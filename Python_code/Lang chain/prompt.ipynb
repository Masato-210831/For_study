{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04781f1-6292-4ef9-ac9d-e2d8c1b20544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d9bd4b-5086-4068-9753-d3dbc93bd00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11561d0a-f5c2-4a4e-864a-3bc1ef373eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-9TMUEHKt0lMEP87J5IhNm0us50iJ9\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1716784978,\n",
      "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nHello! How can I assist you?\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 2,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 11\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.openai.com/v1/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\":\"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"],\n",
    "}\n",
    "\n",
    "data = {\"model\": \"gpt-3.5-turbo-instruct\",\n",
    "        \"prompt\": \"hello!\",\n",
    "        \"temperature\": 0\n",
    "       }\n",
    "\n",
    "response = requests.post(url=url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7803ebc2-9a36-4e31-8513-92900fee1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9RITShurdcU8OfF9bYsGofshnBqB1\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1716292898,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 9,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 18\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\":\"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_KEY\"],\n",
    "}\n",
    "\n",
    "data = {\"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\":\"user\", \"content\":\"hello!\"}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "       }\n",
    "\n",
    "response = requests.post(url=url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063935ce-d0c3-4f80-bc40-b7e21f993194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet langchain==0.0.172 openai==0.27.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fd10fda-3811-4152-9cf7-d9c67fede57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'はじめまして、私はAIアシスタントです。自然言語処理技術を用いて、様々な質問に回答したり、会話をすることができます。お手伝いが必要なことがあれば、遠慮なくお知らせください。どうぞよろしくお願いいたします。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "result = llm.predict(\"自己紹介してください\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9f1f4a-f0e3-414c-9216-0d4e444a5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "result = chat.predict('自己紹介してください')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d4e4081-5f09-4496-aa94-e3ca596f541a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'はじめまして、私はAIアシスタントです。自然言語処理技術を用いて、様々な質問や会話に対応することができます。お手伝いが必要なことがあれば、遠慮なくお知らせください。どうぞよろしくお願いいたします。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4abfefed-babc-44a8-af9a-ddfffd2bcd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "次のコマンドの概要を説明してください。\n",
      "\n",
      "\n",
      "コマンド: ls\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "次のコマンドの概要を説明してください。\n",
    "\n",
    "\n",
    "コマンド: {command}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['command'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "result = prompt.format(command='ls')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da92c4-7e29-413b-aa5c-5a893ad3042d",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22ca8209-9fd1-488e-a80d-a42ea4897738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "次のコマンドの概要を説明してください。\n",
      "\n",
      "\n",
      "コマンド: ls\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lsコマンドは、リスト（list）の略で、指定されたディレクトリ内のファイルやディレクトリの一覧を表示するコマンドです。デフォルトではカレントディレクトリの内容を表示しますが、任意のディレクトリを指定することもできます。lsコマンドを実行することで、どのようなファイルやディレクトリが存在するかを確認することができます。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "次のコマンドの概要を説明してください。\n",
    "\n",
    "\n",
    "コマンド: {command}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['command'],\n",
    "    template= template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "result = chain.run('ls')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e747956-156d-40e8-b904-281832395c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "### 質問 ###\n",
      "私は市場に行って10個のリンゴを買いました。隣人に２つ、修理工に２つ渡しました。それから５つのリンゴを買って１つ食べました。残りは何個ですか？\n",
      "### 質問終了 ###\n",
      "\n",
      "ステップバイステップで考えましょう。\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m1. 最初に市場で10個のリンゴを買いました。\n",
      "2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n",
      "3. その後、5つのリンゴを追加で購入しました。残りは6 + 5 = 11個です。\n",
      "4. 1つのリンゴを食べたので、残りは11 - 1 = 10個です。\n",
      "\n",
      "したがって、残りのリンゴは10個です。\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "入力を結論だけ一言に要約してください。\n",
      "\n",
      "### 入力 ###\n",
      "1. 最初に市場で10個のリンゴを買いました。\n",
      "2. 隣人に2つ、修理工に2つ渡しました。残りは10 - 2 - 2 = 6個です。\n",
      "3. その後、5つのリンゴを追加で購入しました。残りは6 + 5 = 11個です。\n",
      "4. 1つのリンゴを食べたので、残りは11 - 1 = 10個です。\n",
      "\n",
      "したがって、残りのリンゴは10個です。\n",
      "### 入力終了 ###\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m残りのリンゴは10個。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '私は市場に行って10個のリンゴを買いました。隣人に２つ、修理工に２つ渡しました。それから５つのリンゴを買って１つ食べました。残りは何個ですか？',\n",
       " 'output': '残りのリンゴは10個。'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot_template = \"\"\"\n",
    "\n",
    "### 質問 ###\n",
    "{question}\n",
    "### 質問終了 ###\n",
    "\n",
    "ステップバイステップで考えましょう。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cot_prompt = PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    template=cot_template\n",
    ")\n",
    "\n",
    "\n",
    "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n",
    "\n",
    "\n",
    "summarize_template = \"\"\"\n",
    "入力を結論だけ一言に要約してください。\n",
    "\n",
    "### 入力 ###\n",
    "{input}\n",
    "### 入力終了 ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables = ['input'],\n",
    "    template = summarize_template\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n",
    "\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# ここまではモデルの作成\n",
    "cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain])\n",
    "\n",
    "cot_summarize_chain('私は市場に行って10個のリンゴを買いました。隣人に２つ、修理工に２つ渡しました。それから５つのリンゴを買って１つ食べました。残りは何個ですか？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "265957cf-ba23-4f32-b173-beab564fdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "513d6a67-eeee-4913-a557-1cb0c491bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "料理のレシピを教えてください。\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"discription\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"step to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
      "```\n",
      "\n",
      "料理名：カレー\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "  \"ingredients\": [\n",
      "    \"にんじん\",\n",
      "    \"じゃがいも\",\n",
      "    \"玉ねぎ\",\n",
      "    \"牛肉\",\n",
      "    \"カレールー\",\n",
      "    \"水\"\n",
      "  ],\n",
      "  \"steps\": [\n",
      "    \"1. 野菜を切る\",\n",
      "    \"2. 牛肉を炒める\",\n",
      "    \"3. 野菜を加えて炒める\",\n",
      "    \"4. 水を加えて煮る\",\n",
      "    \"5. カレールーを溶かして加える\",\n",
      "    \"6. 煮込んで完成\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: List[str] = Field(discription=\"ingredients of the dish\")\n",
    "    steps : List[str] = Field(description=\"step to make the dish\")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "料理のレシピを教えてください。\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "料理名：{dish}\n",
    "\"\"\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables=[\"dish\"],\n",
    "\n",
    "    # perser.get・・・・で{format_instrucitonsのテンプレートができる}\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "output = chain.run('カレー')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4ea6bdc-1085-4ce8-bc64-96f985fdce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredients=['にんじん', 'じゃがいも', '玉ねぎ', '牛肉', 'カレールー', '水'] steps=['1. 野菜を切る', '2. 牛肉を炒める', '3. 野菜を加えて炒める', '4. 水を加えて煮る', '5. カレールーを溶かして加える', '6. 煮込んで完成']\n",
      "<class '__main__.Recipe'>\n"
     ]
    }
   ],
   "source": [
    "# jsonからpythonのRecipeオブジェクトに変換\n",
    "recipe = parser.parse(output)\n",
    "print(recipe)\n",
    "\n",
    "print(type(recipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41ae7999-9e57-4f08-91ae-f76b301c63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d842355-2ec7-4ebf-8140-818c2bfe8754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  こんにちは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: こんにちは\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:こんにちは！元気ですか？私はAIです。何か質問があればお答えしますよ。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  私は\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: こんにちは\n",
      "AI: こんにちは！元気ですか？私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:私はAIです。何か質問があればお答えしますよ。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  私は中村です\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: こんにちは\n",
      "AI: こんにちは！元気ですか？私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は\n",
      "AI: 私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は中村です\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:中村さん、初めまして！中村さんは何か質問がありますか？私はAIですが、お手伝いできることがあればお知らせくださいね。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  私の\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: こんにちは\n",
      "AI: こんにちは！元気ですか？私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は\n",
      "AI: 私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は中村です\n",
      "AI: 中村さん、初めまして！中村さんは何か質問がありますか？私はAIですが、お手伝いできることがあればお知らせくださいね。\n",
      "Human: 私の\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:申し訳ありませんが、その情報は私のデータベースにはありません。他に何かお手伝いできることがあればお知らせくださいね。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  私の名前を\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: こんにちは\n",
      "AI: こんにちは！元気ですか？私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は\n",
      "AI: 私はAIです。何か質問があればお答えしますよ。\n",
      "Human: 私は中村です\n",
      "AI: 中村さん、初めまして！中村さんは何か質問がありますか？私はAIですが、お手伝いできることがあればお知らせくださいね。\n",
      "Human: 私の\n",
      "AI: 申し訳ありませんが、その情報は私のデータベースにはありません。他に何かお手伝いできることがあればお知らせくださいね。\n",
      "Human: 私の名前を\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:中村さん、お名前を教えていただきありがとうございます。中村さんは何か質問がありますか？私はAIですが、お手伝いできることがあればお知らせくださいね。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39mchat, memory\u001b[38;5;241m=\u001b[39mConversationBufferMemory())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     ai_message \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39muser_message)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, max_tokens=100)\n",
    "conversation = ConversationChain(llm=chat, memory=ConversationBufferMemory())\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    ai_message = conversation.predict(input=user_message)\n",
    "    print(f\"AI:{ai_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5382-3f38-4785-9a30-e7dd0b77e27e",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98f74675-920a-42f7-a64d-d2fb0c7e71dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the terminal to check my current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m/work/Python_code/Lang chain\n",
      "\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "terminal: Run shell commands on this Linux machine.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [terminal]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is your current directory?\n",
      "Thought:I can use the terminal to check my current directory.\n",
      "Action: terminal\n",
      "Action Input: pwd\n",
      "Observation: /work/Python_code/Lang chain\n",
      "\n",
      "Thought:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/langchain/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI now know my current directory.\n",
      "Final Answer: /work/Python_code/Lang chain\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is your current directory?', 'output': '/work/Python_code/Lang chain'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "tools = load_tools(['terminal'], llm=chat)\n",
    "agent_chain = initialize_agent(tools, chat, agent='zero-shot-react-description')\n",
    "\n",
    "result = agent_chain(\"What is your current directory?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "389f5095-7fd2-46c7-ae56-1f35dd3ca656",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39mchat, memory\u001b[38;5;241m=\u001b[39mConversationBufferMemory())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     ai_message \u001b[38;5;241m=\u001b[39m conversation\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39muser_message)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "## この方法ではuserとAIで別々に記憶されていなくて、一連の会話がUserプロンプトに含まれる\n",
    "import openai\n",
    "openai.log = \"debug\"\n",
    "langchain.verbose = True\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0, max_tokens=100)\n",
    "conversation = ConversationChain(llm=chat, memory=ConversationBufferMemory())\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    ai_message = conversation.predict(input=user_message)\n",
    "    print(f\"AI:{ai_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c07a8a69-38c3-4bd7-aecd-0c86ce0b3eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, {\"role\": \"user\", \"content\": \"Hi!I\\'m Masa\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 100, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Masa! How can I assist you today?' additional_kwargs={} example=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=443 request_id=req_f0a7b7889d0e5fa532743e27bf20ac6f response_code=200\n",
      "body='{\\n  \"id\": \"chatcmpl-9RyJ6hI7bE8GpmAZBlfDfZwfnHiQs\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1716453704,\\n  \"model\": \"gpt-3.5-turbo-0125\",\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \"Hello Masa! How can I assist you today?\"\\n      },\\n      \"logprobs\": null,\\n      \"finish_reason\": \"stop\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 21,\\n    \"completion_tokens\": 11,\\n    \"total_tokens\": 32\\n  },\\n  \"system_fingerprint\": null\\n}\\n' headers='{\\'Date\\': \\'Thu, 23 May 2024 08:41:44 GMT\\', \\'Content-Type\\': \\'application/json\\', \\'Transfer-Encoding\\': \\'chunked\\', \\'Connection\\': \\'keep-alive\\', \\'openai-organization\\': \\'user-5a7m6qksriiqjpdqmq7xxwns\\', \\'openai-processing-ms\\': \\'443\\', \\'openai-version\\': \\'2020-10-01\\', \\'strict-transport-security\\': \\'max-age=15724800; includeSubDomains\\', \\'x-ratelimit-limit-requests\\': \\'10000\\', \\'x-ratelimit-limit-tokens\\': \\'60000\\', \\'x-ratelimit-remaining-requests\\': \\'9999\\', \\'x-ratelimit-remaining-tokens\\': \\'59888\\', \\'x-ratelimit-reset-requests\\': \\'8.64s\\', \\'x-ratelimit-reset-tokens\\': \\'112ms\\', \\'x-request-id\\': \\'req_f0a7b7889d0e5fa532743e27bf20ac6f\\', \\'CF-Cache-Status\\': \\'DYNAMIC\\', \\'Server\\': \\'cloudflare\\', \\'CF-RAY\\': \\'8883bfa29942962e-KIX\\', \\'Content-Encoding\\': \\'gzip\\', \\'alt-svc\\': \\'h3=\":443\"; ma=86400\\'}' message='API response body'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Hi!I'm Masa\")\n",
    "]\n",
    "\n",
    "result = chat(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45d4aa78-44cb-4d78-bd21-f69b1da4a060",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     memory\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_user_message(user_message)\n\u001b[1;32m      7\u001b[0m     ai_message \u001b[38;5;241m=\u001b[39m chat(memory\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# user, Ai assistantで別々に記憶させる方法\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"You: \")\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    ai_message = chat(memory.chat_memory.messages)\n",
    "    memory.chat_memory.add_ai_message(ai_message.content)\n",
    "    print(f\"AI: {ai_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d58991-41c9-47bf-be97-8a79e21efdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879eb20-8db4-4124-9994-a391e65a20bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db938e-5cac-4d3f-9122-60c810e08aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9972ae-dceb-4f70-8f7f-46d3a033fef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ade8e-c78a-4f33-a2a1-b12ff5806421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1d815-b110-4d5b-a7cb-7556a98eea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87750dd-9245-4e07-ad78-6ff811dec409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf92647-5265-4e2e-9fb7-1373aa7855bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3643731-bbb3-48a8-a758-94917370ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1657d-4fe2-48dc-bfbc-2ec5be1450ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdfe62-bdd6-44b4-93c9-2de298149362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9406b-365f-460a-ab14-ac7c47e74e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ace73d-cabc-4ae6-9de5-7da62638719c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b453b3-1f29-4306-a16d-59e2076e6a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718669aa-4ee4-44e3-ae42-e4c1c49e2db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a8ec1a-cfd5-4986-803e-17865ebb8ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89616b4a-9932-46e0-b204-0bf6a5c1c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from datetime import datetime\n",
    "from typing import Annotated, Any, Optional\n",
    "import operator\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from retry import retry\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c9696-78b3-4d65-82c4-16071565b7d3",
   "metadata": {},
   "source": [
    "## 他の章で事前定義されたもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de555bd9-06e3-4c30-a566-f65376676c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Goal(BaseModel):\n",
    "    description: str = Field(..., description=\"目標の説明\")\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return f\"{self.description}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36dc312-cf36-44ff-a56a-9c9027f0f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedGoal(BaseModel):\n",
    "    description: str = Field(..., description=\"目標の説明\")\n",
    "    metrics: str = Field(..., description=\"目標の達成度を測定する方法\")\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return f\"{self.description}(測定基準: {self.metrics})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d61adef-562c-478d-a1fc-49dc308e6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassiveGoalCreator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "\n",
    "    def run(self, query: str) -> Goal:\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"ユーザーの入力を分析し、明確で実行可能な目標を生成してください。\\n\"\n",
    "            \"要件:\\n\"\n",
    "            \"1. 目標は具体的かつ明確であり、実行可能なレベルで詳細化されている必要があります。\\n\"\n",
    "            \"2. あなたが実行可能な行動は以下の行動だけです。\\n\"\n",
    "            \"   - インターネットを利用して、目標を達成するための調査を行う。\\n\"\n",
    "            \"   - ユーザーのためのレポートを生成する。\\n\"\n",
    "            \"3. 決して2.以外の行動を取ってはいけません。\\n\"\n",
    "            \"ユーザーの入力: {query}\"\n",
    "        )\n",
    "        chain = prompt | self.llm.with_structured_output(Goal)\n",
    "        return chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b77603-b19a-4d8f-ba5e-e72b96240ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptOptimizer:\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.llm = llm\n",
    "\n",
    "    def run(self, query: str) -> OptimizedGoal:\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"あなたは目標設定の専門家です。以下の目標をSMART原則（Specific: 具体的、Measurable: 測定可能、Achievable: 達成可能、Relevant: 関連性が高い、Time-bound: 期限がある）に基づいて最適化してください。\\n\\n\"\n",
    "            \"元の目標:\\n\"\n",
    "            \"{query}\\n\\n\"\n",
    "            \"指示:\\n\"\n",
    "            \"1. 元の目標を分析し、不足している要素や改善点を特定してください。\\n\"\n",
    "            \"2. あなたが実行可能な行動は以下の行動だけです。\\n\"\n",
    "            \"   - インターネットを利用して、目標を達成するための調査を行う。\\n\"\n",
    "            \"   - ユーザーのためのレポートを生成する。\\n\"\n",
    "            \"3. SMART原則の各要素を考慮しながら、目標を具体的かつ詳細に記載してください。\\n\"\n",
    "            \"   - 一切抽象的な表現を含んではいけません。\\n\"\n",
    "            \"   - 必ず全ての単語が実行可能かつ具体的であることを確認してください。\\n\"\n",
    "            \"4. 目標の達成度を測定する方法を具体的かつ詳細に記載してください。\\n\"\n",
    "            \"5. 元の目標で期限が指定されていない場合は、期限を考慮する必要はありません。\\n\"\n",
    "            \"6. REMEMBER: 決して2.以外の行動を取ってはいけません。\"\n",
    "        )\n",
    "        chain = prompt | self.llm.with_structured_output(OptimizedGoal)\n",
    "        return chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b1d058-3fb5-42b3-bdc5-b6d7eb3906e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseOptimizer:\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        self.llm = llm\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"あなたはAIエージェントシステムのレスポンス最適化スペシャリストです。与えられた目標に対して、エージェントが目標にあったレスポンスを返すためのレスポンス仕様を策定してください。\",\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"以下の手順に従って、レスポンス最適化プロンプトを作成してください：\\n\\n\"\n",
    "                    \"1. 目標分析:\\n\"\n",
    "                    \"提示された目標を分析し、主要な要素や意図を特定してください。\\n\\n\"\n",
    "                    \"2. レスポンス仕様の策定:\\n\"\n",
    "                    \"目標達成のための最適なレスポンス仕様を考案してください。トーン、構造、内容の焦点などを考慮に入れてください。\\n\\n\"\n",
    "                    \"3. 具体的な指示の作成:\\n\"\n",
    "                    \"事前に収集された情報から、ユーザーの期待に沿ったレスポンスをするために必要な、AIエージェントに対する明確で実行可能な指示を作成してください。あなたの指示によってAIエージェントが実行可能なのは、既に調査済みの結果をまとめることだけです。インターネットへのアクセスはできません。\\n\\n\"\n",
    "                    \"4. 例の提供:\\n\"\n",
    "                    \"可能であれば、目標に沿ったレスポンスの例を1つ以上含めてください。\\n\\n\"\n",
    "                    \"5. 評価基準の設定:\\n\"\n",
    "                    \"レスポンスの効果を測定するための基準を定義してください。\\n\\n\"\n",
    "                    \"以下の構造でレスポンス最適化プロンプトを出力してください:\\n\\n\"\n",
    "                    \"目標分析:\\n\"\n",
    "                    \"[ここに目標の分析結果を記入]\\n\\n\"\n",
    "                    \"レスポンス仕様:\\n\"\n",
    "                    \"[ここに策定されたレスポンス仕様を記入]\\n\\n\"\n",
    "                    \"AIエージェントへの指示:\\n\"\n",
    "                    \"[ここにAIエージェントへの具体的な指示を記入]\\n\\n\"\n",
    "                    \"レスポンス例:\\n\"\n",
    "                    \"[ここにレスポンス例を記入]\\n\\n\"\n",
    "                    \"評価基準:\\n\"\n",
    "                    \"[ここに評価基準を記入]\\n\\n\"\n",
    "                    \"では、以下の目標に対するレスポンス最適化プロンプトを作成してください:\\n\"\n",
    "                    \"{query}\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        return chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c44bf-9ce2-4376-93f8-fc083d75960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323e99e-d836-439f-a984-8bbe27fb34cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2e180-dc89-4396-a30d-03f1b0405e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10db89e-0c06-479c-9a09-d4a13222900e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e100bb1-925e-4063-bb53-bf5f7d62ed0e",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd66a48-a562-4c6a-ad8b-3b2aee0cf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposedTasks(BaseModel):\n",
    "    values: list[str] = Field(\n",
    "        default_factory=list,\n",
    "        # min_items=3,\n",
    "        # max_items=5,\n",
    "        description=\"3~5個に分解されたタスク\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ReflectiveAgentState(BaseModel):\n",
    "    query: str = Field(..., description=\"ユーザーが最初に入力したクエリ\")\n",
    "    optimized_goal: str = Field(default=\"\", description=\"最適化された目標\")\n",
    "    optimized_response: str = Field(\n",
    "        default=\"\", description=\"最適化されたレスポンス定義\"\n",
    "    )\n",
    "    tasks: list[str] = Field(default_factory=list, description=\"実行するタスクのリスト\")\n",
    "    current_task_index: int = Field(default=0, description=\"現在実行中のタスクの番号\")\n",
    "    results: Annotated[list[str], operator.add] = Field(\n",
    "        default_factory=list, description=\"実行済みタスクの結果リスト\"\n",
    "    )\n",
    "    reflection_ids: Annotated[list[str], operator.add] = Field(\n",
    "        default_factory=list, description=\"リフレクション結果のIDリスト\"\n",
    "    )\n",
    "    final_output: str = Field(default=\"\", description=\"最終的な出力結果\")\n",
    "    retry_count: int = Field(default=0, description=\"タスクの再試行回数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081b89d-aefe-4230-9b9c-dd88b4c84246",
   "metadata": {},
   "source": [
    "## Reflector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe04cad3-da61-4d1e-802a-ff759141247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionJudgment(BaseModel):\n",
    "    needs_retry: bool = Field(\n",
    "        description=\"タスクの実行結果は適切だったと思いますか?あなたの判断を真偽値で示してください。\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        description=\"あなたの判断に対するあなたの自信の度合いを0から1までの小数で示してください。\"\n",
    "    )\n",
    "    reasons: list[str] = Field(\n",
    "        description=\"タスクの実行結果の適切性とそれに対する自信度について、判断に至った理由を簡潔に列挙してください。\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a46df24-d97b-404f-861e-c3d31a8e1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    id: str = Field(description=\"リフレクション内容に一意性を与えるためのID\")\n",
    "    task: str = Field(description=\"ユーザーから与えられたタスクの内容\")\n",
    "    reflection: str = Field(\n",
    "        description=\"このタスクに取り組んだ際のあなたの思考プロセスを振り返ってください。何か改善できる点はありましたか? 次に同様のタスクに取り組む際に、より良い結果を出すための教訓を2〜3文程度で簡潔に述べてください。\"\n",
    "    )\n",
    "    judgment: ReflectionJudgment = Field(description=\"リトライが必要かどうかの判定\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f5aac6-f9df-4b90-a54b-99167196bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionManager:\n",
    "    def __init__(self, file_path: str = \"tmp/reflection_db.json\"):\n",
    "        self.file_path = file_path\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        self.reflections: dict[str, Reflection] = {}\n",
    "        self.embeddings_dict: dict[str, list[float]] = {}\n",
    "        self.index = None\n",
    "        self.load_reflections()\n",
    "\n",
    "    def load_reflections(self):\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                for item in data:\n",
    "                    reflection = Reflection(**item[\"reflection\"])\n",
    "                    self.reflections[reflection.id] = reflection\n",
    "                    self.embeddings_dict[reflection.id] = item[\"embedding\"]\n",
    "\n",
    "            if self.reflections:\n",
    "                embeddings = list(self.embeddings_dict.values())\n",
    "                self.index = faiss.IndexFlatL2(len(embeddings[0]))\n",
    "                self.index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "    def save_reflection(self, reflection: Reflection) -> str:\n",
    "        reflection.id = str(uuid.uuid4())\n",
    "        reflection_id = reflection.id\n",
    "        self.reflections[reflection_id] = reflection\n",
    "        embedding = self.embeddings.embed_query(reflection.reflection)\n",
    "        self.embeddings_dict[reflection_id] = embedding\n",
    "\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(len(embedding))\n",
    "        self.index.add(np.array([embedding]).astype(\"float32\"))\n",
    "\n",
    "        with open(self.file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(\n",
    "                [\n",
    "                    {\"reflection\": reflection.dict(), \"embedding\": embedding}\n",
    "                    for reflection, embedding in zip(\n",
    "                        self.reflections.values(), self.embeddings_dict.values()\n",
    "                    )\n",
    "                ],\n",
    "                file,\n",
    "                ensure_ascii=False,\n",
    "                indent=4,\n",
    "            )\n",
    "\n",
    "        return reflection_id\n",
    "\n",
    "    def get_reflection(self, reflection_id: str) -> Optional[Reflection]:\n",
    "        return self.reflections.get(reflection_id)\n",
    "\n",
    "    def get_relevant_reflections(self, query: str, k: int = 3) -> list[Reflection]:\n",
    "        if not self.reflections or self.index is None:\n",
    "            return []\n",
    "\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        try:\n",
    "            D, I = self.index.search(\n",
    "                np.array([query_embedding]).astype(\"float32\"),\n",
    "                min(k, len(self.reflections)),\n",
    "            )\n",
    "            reflection_ids = list(self.reflections.keys())\n",
    "            return [\n",
    "                self.reflections[reflection_ids[i]]\n",
    "                for i in I[0]\n",
    "                if i < len(reflection_ids)\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reflection search: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acac32c-9255-4b57-9d24-72da8e5b9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskReflector:\n",
    "    def __init__(self, llm: BaseChatModel, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm.with_structured_output(Reflection)\n",
    "        self.reflection_manager = reflection_manager\n",
    "\n",
    "    def run(self, task: str, result: str) -> Reflection:\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"与えられたタスクの内容:\\n{task}\\n\\n\"\n",
    "            \"タスクを実行した結果:\\n{result}\\n\\n\"\n",
    "            \"あなたは高度な推論能力を持つAIエージェントです。上記のタスクを実行した結果を分析し、このタスクに対するあなたの取り組みが適切だったかどうかを内省してください。\\n\"\n",
    "            \"以下の項目に沿って、リフレクションの内容を出力してください。\\n\\n\"\n",
    "            \"リフレクション:\\n\"\n",
    "            \"このタスクに取り組んだ際のあなたの思考プロセスや方法を振り返ってください。何か改善できる点はありましたか?\\n\"\n",
    "            \"次に同様のタスクに取り組む際に、より良い結果を出すための教訓を2〜3文程度で簡潔に述べてください。\\n\\n\"\n",
    "            \"判定:\\n\"\n",
    "            \"- 結果の適切性: タスクの実行結果は適切だったと思いますか?あなたの判断を真偽値で示してください。\\n\"\n",
    "            \"- 判定の自信度: 上記の判断に対するあなたの自信の度合いを0から1までの小数で示してください。\\n\"\n",
    "            \"- 判定の理由: タスクの実行結果の適切性とそれに対する自信度について、判断に至った理由を簡潔に列挙してください。\\n\\n\"\n",
    "            \"出力は必ず日本語で行ってください。\\n\\n\"\n",
    "            \"Tips: Make sure to answer in the correct format.\"\n",
    "        )\n",
    "\n",
    "        chain = prompt | self.llm\n",
    "\n",
    "        @retry(tries=5)\n",
    "        def invoke_chain() -> Reflection:\n",
    "            return chain.invoke({\"task\": task, \"result\": result})\n",
    "\n",
    "        reflection = invoke_chain()\n",
    "        reflection_id = self.reflection_manager.save_reflection(reflection)\n",
    "        reflection.id = reflection_id\n",
    "\n",
    "        return reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f94b3da-ff75-412a-8856-6b29e8abdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reflections(reflections: list[Reflection]) -> str:\n",
    "    return (\n",
    "        \"\\n\\n\".join(\n",
    "            f\"<ref_{i}><task>{r.task}</task><reflection>{r.reflection}</reflection></ref_{i}>\"\n",
    "            for i, r in enumerate(reflections)\n",
    "        )\n",
    "        if reflections\n",
    "        else \"No relevant past reflections.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da0dc4c-798b-4c91-8d80-fffb75cb073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveGoalCreator:\n",
    "    def __init__(self, llm: ChatOpenAI, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm\n",
    "        self.reflection_manager = reflection_manager\n",
    "        self.passive_goal_creator = PassiveGoalCreator(llm=self.llm)\n",
    "        self.prompt_optimizer = PromptOptimizer(llm=self.llm)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        relevant_reflections = self.reflection_manager.get_relevant_reflections(query)\n",
    "        reflection_text = format_reflections(relevant_reflections)\n",
    "\n",
    "        query = f\"{query}\\n\\n目標設定する際に以下の過去のふりかえりを考慮すること:\\n{reflection_text}\"\n",
    "        goal: Goal = self.passive_goal_creator.run(query=query)\n",
    "        optimized_goal: OptimizedGoal = self.prompt_optimizer.run(query=goal.text)\n",
    "        return optimized_goal.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8f359d2-10a2-4473-b335-281bd6643133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveResponseOptimizer:\n",
    "    def __init__(self, llm: ChatOpenAI, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm\n",
    "        self.reflection_manager = reflection_manager\n",
    "        self.response_optimizer = ResponseOptimizer(llm=llm)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        relevant_reflections = self.reflection_manager.get_relevant_reflections(query)\n",
    "        reflection_text = format_reflections(relevant_reflections)\n",
    "\n",
    "        query = f\"{query}\\n\\nレスポンス最適化に以下の過去のふりかえりを考慮すること:\\n{reflection_text}\"\n",
    "        optimized_response: str = self.response_optimizer.run(query=query)\n",
    "        return optimized_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739d2b92-e418-4bb0-b888-ae0985615f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDecomposer:\n",
    "    def __init__(self, llm: ChatOpenAI, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm.with_structured_output(DecomposedTasks)\n",
    "        self.current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        self.reflection_manager = reflection_manager\n",
    "\n",
    "    def run(self, query: str) -> DecomposedTasks:\n",
    "        relevant_reflections = self.reflection_manager.get_relevant_reflections(query)\n",
    "        reflection_text = format_reflections(relevant_reflections)\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            f\"CURRENT_DATE: {self.current_date}\\n\"\n",
    "            \"-----\\n\"\n",
    "            \"タスク: 与えられた目標を具体的で実行可能なタスクに分解してください。\\n\"\n",
    "            \"要件:\\n\"\n",
    "            \"1. 以下の行動だけで目標を達成すること。決して指定された以外の行動をとらないこと。\\n\"\n",
    "            \"   - インターネットを利用して、目標を達成するための調査を行う。\\n\"\n",
    "            \"2. 各タスクは具体的かつ詳細に記載されており、単独で実行ならびに検証可能な情報を含めること。一切抽象的な表現を含まないこと。\\n\"\n",
    "            \"3. タスクは実行可能な順序でリスト化すること。\\n\"\n",
    "            \"4. タスクは日本語で出力すること。\\n\"\n",
    "            \"5. タスクを作成する際に以下の過去のふりかえりを考慮すること:\\n{reflections}\\n\\n\"\n",
    "            \"目標: {query}\"\n",
    "        )\n",
    "        chain = prompt | self.llm\n",
    "        tasks = chain.invoke({\"query\": query, \"reflections\": reflection_text})\n",
    "        return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeee57b6-4ce4-4932-b7c4-69f246021fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskExecutor:\n",
    "    def __init__(self, llm: ChatOpenAI, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm\n",
    "        self.reflection_manager = reflection_manager\n",
    "        self.current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        self.tools = [TavilySearchResults(max_results=3)]\n",
    "\n",
    "    def run(self, task: str) -> str:\n",
    "        relevant_reflections = self.reflection_manager.get_relevant_reflections(task)\n",
    "        reflection_text = format_reflections(relevant_reflections)\n",
    "        agent = create_react_agent(self.llm, self.tools)\n",
    "        result = agent.invoke(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    (\n",
    "                        \"human\",\n",
    "                        f\"CURRENT_DATE: {self.current_date}\\n\"\n",
    "                        \"-----\\n\"\n",
    "                        f\"次のタスクを実行し、詳細な回答を提供してください。\\n\\nタスク: {task}\\n\\n\"\n",
    "                        \"要件:\\n\"\n",
    "                        \"1. 必要に応じて提供されたツールを使用すること。\\n\"\n",
    "                        \"2. 実行において徹底的かつ包括的であること。\\n\"\n",
    "                        \"3. 可能な限り具体的な事実やデータを提供すること。\\n\"\n",
    "                        \"4. 発見事項を明確に要約すること。\\n\"\n",
    "                        f\"5. 以下の過去のふりかえりを考慮すること:\\n{reflection_text}\\n\",\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        return result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a487cb35-b0c8-4845-a5c1-28375b6d2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultAggregator:\n",
    "    def __init__(self, llm: ChatOpenAI, reflection_manager: ReflectionManager):\n",
    "        self.llm = llm\n",
    "        self.reflection_manager = reflection_manager\n",
    "        self.current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        query: str,\n",
    "        results: list[str],\n",
    "        reflection_ids: list[str],\n",
    "        response_definition: str,\n",
    "    ) -> str:\n",
    "        relevant_reflections = [\n",
    "            self.reflection_manager.get_reflection(rid) for rid in reflection_ids\n",
    "        ]\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"与えられた目標:\\n{query}\\n\\n\"\n",
    "            \"調査結果:\\n{results}\\n\\n\"\n",
    "            \"与えられた目標に対し、調査結果を用いて、以下の指示に基づいてレスポンスを生成してください。\\n\"\n",
    "            \"{response_definition}\\n\\n\"\n",
    "            \"過去のふりかえりを考慮すること:\\n{reflection_text}\\n\"\n",
    "        )\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        return chain.invoke(\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"results\": \"\\n\\n\".join(\n",
    "                    f\"Info {i+1}:\\n{result}\" for i, result in enumerate(results)\n",
    "                ),\n",
    "                \"response_definition\": response_definition,\n",
    "                \"reflection_text\": format_reflections(relevant_reflections),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a40e56bc-8d03-445e-b3ed-e9e2ba6eea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: ChatOpenAI,\n",
    "        reflection_manager: ReflectionManager,\n",
    "        task_reflector: TaskReflector,\n",
    "        max_retries: int = 2,\n",
    "    ):\n",
    "        self.reflection_manager = reflection_manager\n",
    "        self.task_reflector = task_reflector\n",
    "        self.reflective_goal_creator = ReflectiveGoalCreator(\n",
    "            llm=llm, reflection_manager=self.reflection_manager\n",
    "        )\n",
    "        self.reflective_response_optimizer = ReflectiveResponseOptimizer(\n",
    "            llm=llm, reflection_manager=self.reflection_manager\n",
    "        )\n",
    "        self.query_decomposer = QueryDecomposer(\n",
    "            llm=llm, reflection_manager=self.reflection_manager\n",
    "        )\n",
    "        self.task_executor = TaskExecutor(\n",
    "            llm=llm, reflection_manager=self.reflection_manager\n",
    "        )\n",
    "        self.result_aggregator = ResultAggregator(\n",
    "            llm=llm, reflection_manager=self.reflection_manager\n",
    "        )\n",
    "        self.max_retries = max_retries\n",
    "        self.graph = self._create_graph()\n",
    "\n",
    "    def _create_graph(self) -> StateGraph:\n",
    "        graph = StateGraph(ReflectiveAgentState)\n",
    "        graph.add_node(\"goal_setting\", self._goal_setting)\n",
    "        graph.add_node(\"decompose_query\", self._decompose_query)\n",
    "        graph.add_node(\"execute_task\", self._execute_task)\n",
    "        graph.add_node(\"reflect_on_task\", self._reflect_on_task)\n",
    "        graph.add_node(\"update_task_index\", self._update_task_index)\n",
    "        graph.add_node(\"aggregate_results\", self._aggregate_results)\n",
    "        graph.set_entry_point(\"goal_setting\")\n",
    "        graph.add_edge(\"goal_setting\", \"decompose_query\")\n",
    "        graph.add_edge(\"decompose_query\", \"execute_task\")\n",
    "        graph.add_edge(\"execute_task\", \"reflect_on_task\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"reflect_on_task\",\n",
    "            self._should_retry_or_continue,\n",
    "            {\n",
    "                \"retry\": \"execute_task\",\n",
    "                \"continue\": \"update_task_index\",\n",
    "                \"finish\": \"aggregate_results\",\n",
    "            },\n",
    "        )\n",
    "        graph.add_edge(\"update_task_index\", \"execute_task\")\n",
    "        graph.add_edge(\"aggregate_results\", END)\n",
    "        return graph.compile()\n",
    "\n",
    "    def _goal_setting(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        print('goal_setting started')\n",
    "        optimized_goal: str = self.reflective_goal_creator.run(query=state.query)\n",
    "        optimized_response: str = self.reflective_response_optimizer.run(\n",
    "            query=optimized_goal\n",
    "        )\n",
    "        print('goal_setting finished')\n",
    "        return {\n",
    "            \"optimized_goal\": optimized_goal,\n",
    "            \"optimized_response\": optimized_response,\n",
    "        }\n",
    "\n",
    "    def _decompose_query(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        print('decompose_query started')\n",
    "        tasks: DecomposedTasks = self.query_decomposer.run(query=state.optimized_goal)\n",
    "        print('decompose_query finished')\n",
    "        \n",
    "        return {\"tasks\": tasks.values}\n",
    "\n",
    "    def _execute_task(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        print('execute_task started')\n",
    "        current_task = state.tasks[state.current_task_index]\n",
    "        result = self.task_executor.run(task=current_task)\n",
    "        print('execute_task finished')\n",
    "        return {\"results\": [result], \"current_task_index\": state.current_task_index}\n",
    "\n",
    "    def _reflect_on_task(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        print('reflect_on_task started')\n",
    "        current_task = state.tasks[state.current_task_index]\n",
    "        current_result = state.results[-1]\n",
    "        reflection = self.task_reflector.run(task=current_task, result=current_result)\n",
    "        print('reflect_on_task finished')\n",
    "        return {\n",
    "            \"reflection_ids\": [reflection.id],\n",
    "            \"retry_count\": (\n",
    "                state.retry_count + 1 if reflection.judgment.needs_retry else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _should_retry_or_continue(self, state: ReflectiveAgentState) -> str:\n",
    "        print('should_retry_or_continue started')\n",
    "        print('current_task_id',state.current_task_index ,' retry_conunt: ', state.retry_count)\n",
    "        latest_reflection_id = state.reflection_ids[-1]\n",
    "        latest_reflection = self.reflection_manager.get_reflection(latest_reflection_id)\n",
    "        if (\n",
    "            latest_reflection\n",
    "            and latest_reflection.judgment.needs_retry\n",
    "            and state.retry_count < self.max_retries\n",
    "        ):\n",
    "            print('should_retry_or_continue finished')\n",
    "            return \"retry\"\n",
    "        elif state.current_task_index < len(state.tasks) - 1:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"finish\"\n",
    "\n",
    "    def _update_task_index(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        return {\"current_task_index\": state.current_task_index + 1}\n",
    "\n",
    "    def _aggregate_results(self, state: ReflectiveAgentState) -> dict[str, Any]:\n",
    "        final_output = self.result_aggregator.run(\n",
    "            query=state.optimized_goal,\n",
    "            results=state.results,\n",
    "            reflection_ids=state.reflection_ids,\n",
    "            response_definition=state.optimized_response,\n",
    "        )\n",
    "        return {\"final_output\": final_output}\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        initial_state = ReflectiveAgentState(query=query)\n",
    "        final_state = self.graph.invoke(initial_state, {\"recursion_limit\": 1000})\n",
    "        return final_state.get(\"final_output\", \"エラー: 出力に失敗しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee9bd5d-5da2-456f-b594-22adb34bf440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal_setting started\n",
      "goal_setting finished\n",
      "decompose_query started\n",
      "decompose_query finished\n",
      "execute_task started\n",
      "execute_task finished\n",
      "reflect_on_task started\n",
      "reflect_on_task finished\n",
      "should_retry_or_continue started\n",
      "current_task_id 0  retry_conunt:  0\n",
      "execute_task started\n",
      "execute_task finished\n",
      "reflect_on_task started\n",
      "reflect_on_task finished\n",
      "should_retry_or_continue started\n",
      "current_task_id 1  retry_conunt:  0\n",
      "execute_task started\n",
      "execute_task finished\n",
      "reflect_on_task started\n",
      "reflect_on_task finished\n",
      "should_retry_or_continue started\n",
      "current_task_id 2  retry_conunt:  0\n",
      "execute_task started\n",
      "execute_task finished\n",
      "reflect_on_task started\n",
      "reflect_on_task finished\n",
      "should_retry_or_continue started\n",
      "current_task_id 3  retry_conunt:  0\n",
      "execute_task started\n",
      "execute_task finished\n",
      "reflect_on_task started\n",
      "reflect_on_task finished\n",
      "should_retry_or_continue started\n",
      "current_task_id 4  retry_conunt:  0\n",
      "以下に、商用利用可能な日本語対応の視覚言語モデル（VLM）を5つリストアップし、それぞれの特徴、利点、制限を詳細に分析した内容を示します。\n",
      "\n",
      "### モデル一覧\n",
      "\n",
      "1. **Japanese Stable VLM**\n",
      "   - **特徴**: Stability AIが開発した日本語対応の視覚言語モデル。画像に対する質問にチャット形式で応答する機能を持つ。\n",
      "   - **利点**: 高精度な日本語対応、オープンソースで商用利用可能、Google Colabで試用可能。\n",
      "   - **制限**: 商用利用には条件がある、技術的知識が必要、データセットの偏りの可能性。\n",
      "\n",
      "2. **サイバーエージェントVLM**\n",
      "   - **特徴**: 75億パラメータの大規模視覚言語モデル。Hugging Faceで商用利用可能。\n",
      "   - **利点**: 大規模なパラメータ数による高精度な処理、商用利用可能。\n",
      "   - **制限**: 大規模な計算資源が必要、特定の用途に特化していない。\n",
      "\n",
      "3. **AIdeaLab VideoJP**\n",
      "   - **特徴**: 動画生成に特化したAI基盤モデル。日本語と英語の文章がそのまま通じる。\n",
      "   - **利点**: 幅広い用途での活用が可能、学習データの透明性を重視。\n",
      "   - **制限**: 現時点では2秒間の短尺動画に限定、著作権やライセンスに配慮が必要。\n",
      "\n",
      "4. **LSPT-VLM**\n",
      "   - **特徴**: 東京理科大学が開発した大規模事前学習済み視覚-言語モデル。視覚と言語の統合に新たな構造を持つ。\n",
      "   - **利点**: 高度なタスクの実行が可能、最新の研究成果を反映。\n",
      "   - **制限**: 商用利用の詳細が不明、技術的なハードルが高い。\n",
      "\n",
      "5. **Sakana AI VLM**\n",
      "   - **特徴**: 自律的にタスクを理解し、リアルタイムに適応する能力を持つモデル。\n",
      "   - **利点**: 自律的なタスク適応能力、最新の技術を反映。\n",
      "   - **制限**: 商用利用の条件が不明、技術的な知識が必要。\n",
      "\n",
      "### 比較表\n",
      "\n",
      "| モデル名              | 特徴                                                                 | 利点                                                                 | 制限                                                                 |\n",
      "|-----------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
      "| Japanese Stable VLM   | 画像に対する質問応答、オープンソース                                 | 高精度な日本語対応、商用利用可能                                     | 商用利用条件、技術的知識必要                                         |\n",
      "| サイバーエージェントVLM | 75億パラメータ、Hugging Faceで利用可能                               | 高精度な処理、商用利用可能                                           | 大規模計算資源必要                                                   |\n",
      "| AIdeaLab VideoJP      | 動画生成特化、日本語と英語対応                                       | 幅広い用途、学習データの透明性                                       | 短尺動画限定、著作権配慮必要                                         |\n",
      "| LSPT-VLM              | 視覚と言語の統合、新たな構造                                         | 高度なタスク実行可能                                                 | 商用利用不明、技術的ハードル高                                       |\n",
      "| Sakana AI VLM         | 自律的タスク理解、リアルタイム適応                                   | 自律的適応能力、最新技術反映                                         | 商用利用条件不明、技術的知識必要                                     |\n",
      "\n",
      "### 推奨モデル: AIdeaLab VideoJP\n",
      "**選定理由**: AIdeaLab VideoJPは、特にマーケティングや教育分野での利用が期待される幅広い用途に対応しており、日本語と英語の両方に対応しているため、国際的なプロジェクトにも適しています。また、学習データの透明性を重視しており、著作権やライセンスに配慮した運用が可能です。短尺動画の生成に特化しているため、現代のデジタルコンテンツ市場において非常に有用です。\n",
      "\n",
      "### 評価基準\n",
      "1. **モデルの特定数**: 5つのモデルがリストアップされています。\n",
      "2. **情報の網羅性**: 各モデルについて、公式サイト、技術文書、レビュー記事からの情報が含まれています。\n",
      "3. **レポートの完成度**: 各モデルの特徴、利点、制限が詳細に分析され、比較表と推奨モデルの選定理由が明記されています。\n",
      "4. **ユーザーのフィードバック**: レポートの内容がユーザーのニーズに合致し、80%以上の満足度を得ることを目指します。\n"
     ]
    }
   ],
   "source": [
    "task = '商用利用可能なローカルで使える日本語対応のVLMはありますか？'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\", temperature=0.0\n",
    "    )\n",
    "reflection_manager = ReflectionManager(file_path=\"tmp/self_reflection_db.json\")\n",
    "task_reflector = TaskReflector(llm=llm, reflection_manager=reflection_manager)\n",
    "agent = ReflectiveAgent(\n",
    "    llm=llm, reflection_manager=reflection_manager, task_reflector=task_reflector\n",
    ")\n",
    "result = agent.run(task)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630b992-de81-462a-b003-578769566151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb435e3f-1207-4f28-90d3-bf7c994ad2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac84d48-c6cc-4eb0-9663-eb27fcf3a5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bc960-bd90-409e-a848-0867d6c0a67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb7f63-01a3-4959-a464-aff35d51f72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dcf0f-c333-4285-8828-76561683943d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

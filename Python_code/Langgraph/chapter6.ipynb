{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ab89b5-4905-4b28-91d6-332c511f5918",
   "metadata": {},
   "source": [
    "## Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e62bc58d-43a2-482c-aae7-0cd4e23dba94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92ad3e1-eb80-4824-bf2e-eb5f8ed35ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a02377fc-a6ab-410b-a6e9-99d994d76a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'cookbook/sql_db_qa.mdx',\n",
       " 'file_path': 'cookbook/sql_db_qa.mdx',\n",
       " 'file_name': 'sql_db_qa.mdx',\n",
       " 'file_type': '.mdx'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f7b9c44-5750-489b-acb0-99908286c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3e067d-6666-4e6b-9dd9-6aeab03792fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクターストアなど）に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。これにより、開発者は異なるプロバイダー間での切り替えが容易になります。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cae45-b9ef-4f10-9efd-e073c3fd3536",
   "metadata": {},
   "source": [
    "## 6.3. 検索クエリの工夫\n",
    "### HyDE（Hypothetical Document Embeddings）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bb07630-a411-4b6b-a5b5-9236ddb6c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "次の質問に回答する一文を書いてください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c76be1-8c40-46ff-872a-c4eb84cf3f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、アプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能を提供しています。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築することができます。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなプロバイダーと統合し、標準インターフェースを実装しているため、異なるコンポーネントプロバイダーを簡単に切り替えることができます。また、LangGraphを使用することで、複雑なアプリケーションのオーケストレーションが可能になり、記憶やストリーミングなどの重要な機能をサポートします。LangSmithは、アプリケーションのトレースや評価を行うためのプラットフォームです。\\n\\n全体として、LangChainは開発者がLLMアプリケーションを効率的に構築、評価、デプロイできるように設計されています。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": hypothetical_chain | retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2843d214-a25c-44fc-a3e3-7741859328b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、言語モデルを活用したアプリケーションの開発を容易にするためのフレームワークで、データの取得、処理、出力の各ステップを統合し、対話型エージェントや自動化ツールの構築をサポートします。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2737aa-d5f7-4c66-8e37-1648ad008aaa",
   "metadata": {},
   "source": [
    "## 複数の検索クエリの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d079e6e-72c3-4057-b31a-d29ce64b2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に対してベクターデータベースから関連文書を検索するために、\n",
    "3つの異なる検索クエリを生成してください。\n",
    "距離ベースの類似性検索の限界を克服するために、\n",
    "ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d5aa528-68d9-4be6-8e22-b01ced397b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、開発、運用、デプロイの各段階を簡素化することを目的としています。具体的には、以下のような特徴があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **運用**: LangSmithを使用してアプリケーションを監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、さまざまなモデルや関連コンポーネントの標準化されたインターフェースを提供し、開発者が異なるプロバイダー間で簡単に切り替えたり、コンポーネントを組み合わせたりできるようにします。また、アプリケーションの複雑さが増すにつれて、オーケストレーションや可視化、評価のニーズが高まることに対応しています。\\n\\n全体として、LangChainは、開発者がAIアプリケーションを効率的に構築、運用、デプロイできるようにするための強力なエコシステムを提供しています。'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map(),\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9746c3f9-dc1f-4088-8811-666487d964fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChainとは何か、その基本的な機能と用途について',\n",
       " 'LangChainの主要なコンポーネントとアーキテクチャの説明',\n",
       " 'LangChainを使用した具体的なアプリケーション例とその利点']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_generation_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e25dd79-a986-4bb6-a55b-f63eddfda5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    "    | retriever.map()\n",
    "    # | (lambda docs_list: [doc.page_content for doc in docs_list[0]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a1e0756-8cb9-4350-a186-98959cbd7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_results_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48032bb1-81f4-4400-a701-4a3feaff8199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cacc93c2-db46-4136-95c6-9ed712baa8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([-18, -18, -18])\n",
    "array2 = np.array([0, 0, 0])\n",
    "\n",
    "# 連結\n",
    "result = np.concatenate((array1, array2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "362e728c-08ed-4fdc-8e93-3e9e8899ce38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18, -18, -18,   0,   0,   0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e45ef1f6-ac9b-4d34-9683-622cfe70edc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。LangGraphを利用することで、状態を持つエージェントを作成し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの関連技術に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、開発者は必要なコンポーネントを選択して使用することができます。'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
    "    question = inp['question']\n",
    "    documents = inp['documents']\n",
    "\n",
    "    cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=top_n)\n",
    "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"documents\": retriever,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=rerank)\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "rerank_rag_chain.invoke('Langchainの概要を教えて')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01e37de9-3735-4d71-b129-2a3eda78e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# langsmithでトレースしやすいようにconfigをつける\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"langchain_document_retriever\"}\n",
    ")\n",
    "\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config(\n",
    "    {\"run_name\": \"web_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "228ed54e-3ed9-4db8-b30f-7a8c0e241aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = 'web'\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt\n",
    "    | model.with_structured_output(RouteOutput)\n",
    "    | (lambda x: x.route)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "29bd2689-eba2-42ce-a0c1-3cbb117b173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Route(Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = 'web'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a8643c1-b2cb-42f6-a3b0-239185d89ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(Test.web, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a3fda12-eb9b-4037-bddd-3b3ae8ec6085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(Route.web, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d5b43e4-d0d3-4b38-a50e-3e3e6afa0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown retriever: {retriever}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9627a1b8-caf5-4f11-93b6-05c16d66b229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLM）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能があります。\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を使用してアプリケーションを構築できます。また、LangGraphを利用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **生産化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイ**: LangGraphアプリケーションを生産準備が整ったAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクターストアなど）に対する標準インターフェースを実装しており、数百のプロバイダーと統合されています。また、複数のオープンソースライブラリで構成されており、特に`langchain-core`、統合パッケージ、`langchain`、`langchain-community`、`langgraph`などが含まれています。\\n\\nさらに、LangChainは、開発者がアプリケーションを構築する際に直面するさまざまな課題に対処するための標準化されたコンポーネントインターフェース、オーケストレーション機能、可視性と評価のサポートを提供しています。'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9c0e988-747a-46e5-95da-bbe6ebbeb6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'東京の今日の天気は、10月27日（日）で最高気温24℃、最低気温17℃、降水確率は50%です。'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"東京の今日の天気は？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c34a48-c80c-4b7e-9317-8d4dc123c2d0",
   "metadata": {},
   "source": [
    "## ハイブリッド検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc82288a-ab80-4945-a746-8e534fbf8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # 各ドキュメントのコンテンツ (文字列) とそのスコアの対応を保持する辞書を準備\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # 検索クエリごとにループ\n",
    "    for docs in retriever_outputs:\n",
    "        # 検索結果のドキュメントごとにループ\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "\n",
    "            # 初めて登場したコンテンツの場合はスコアを0で初期化\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1 / (順位 + k)) のスコアを加算\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # スコアの大きい順にソート\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "993eced8-9d11-4abe-ac99-7e3173daac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"chroma_retriever\"}\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
    "    {\"run_name\": \"bm25_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "649f724a-a358-4162-bc61-9c914c2d8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel({\n",
    "        \"chroma_documents\": chroma_retriever,\n",
    "        \"bm25_documents\": bm25_retriever,\n",
    "    })\n",
    "    # | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    # | reciprocal_rank_fusion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3aa9383f-2c2d-479d-8ac9-e55fa86ad400",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m hybrid_rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# | prompt | model | StrOutputParser()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mhybrid_rag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangChainの概要を教えて\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e8e91d6d-e8bc-4b6f-b430-7a1487c0ecf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chroma_documents': [Document(metadata={'file_name': 'introduction.mdx', 'file_path': 'docs/docs/introduction.mdx', 'file_type': '.mdx', 'source': 'docs/docs/introduction.mdx'}, page_content='---\\nsidebar_position: 0\\nsidebar_class_name: hidden\\n---\\n\\n# Introduction\\n\\n**LangChain** is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n- **Development**: Build your applications using LangChain\\'s open-source [components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/).\\nUse [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.\\n- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\n- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).\\n\\nimport ThemedImage from \\'@theme/ThemedImage\\';\\nimport useBaseUrl from \\'@docusaurus/useBaseUrl\\';\\n\\n<ThemedImage\\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\\n  sources={{\\n    light: useBaseUrl(\\'/svg/langchain_stack_112024.svg\\'),\\n    dark: useBaseUrl(\\'/svg/langchain_stack_112024_dark.svg\\'),\\n  }}\\n  style={{ width: \"100%\" }}\\n  title=\"LangChain Framework Overview\"\\n/>\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the [integrations](/docs/integrations/providers/) page for\\nmore.\\n\\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\\n\\n<ChatModelTabs/>\\n\\n```python\\nmodel.invoke(\"Hello, world!\")\\n```\\n\\n:::note\\n\\nThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.\\n\\n:::\\n\\n## Architecture\\n\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\n[Architecture](/docs/concepts/architecture/) page.\\n\\n- **`langchain-core`**: Base abstractions for chat models and other components.\\n- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\n- **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\n- **`langchain-community`**: Third-party integrations that are community maintained.\\n- **`langgraph`**: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).\\n\\n## Guides\\n\\n### [Tutorials](/docs/tutorials)\\n\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials).\\nThis is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\n- [Build a Simple LLM Application](/docs/tutorials/llm_chain)\\n- [Build a Chatbot](/docs/tutorials/chatbot)\\n- [Build an Agent](/docs/tutorials/agents)\\n- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\\n\\nExplore the full list of LangChain tutorials [here](/docs/tutorials), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).\\n\\n\\n### [How-to guides](/docs/how_to)\\n\\n[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the [Tutorials](/docs/tutorials) and the [API Reference](https://python.langchain.com/api_reference/).\\nHowever, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models),\\n[vector stores](/docs/how_to/#vector-stores), and other common LangChain components.\\n\\nCheck out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).\\n\\n### [Conceptual guide](/docs/concepts)\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) you\\'ll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).\\n\\n### [Integrations](integrations/providers/index.mdx)\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/),\\nor other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).\\n\\n\\n### [API reference](https://python.langchain.com/api_reference/)\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\n## Ecosystem\\n\\n### [🦜🛠️ LangSmith](https://docs.smith.langchain.com)\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n### [🦜🕸️ LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\n## Additional resources\\n\\n### [Versions](/docs/versions/v0_3/)\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\n### [Security](/docs/security)\\nRead up on [security](/docs/security) best practices to make sure you\\'re developing safely with LangChain.\\n\\n### [Contributing](contributing/index.mdx)\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.\\n'),\n",
       "  Document(metadata={'file_name': 'lcel.mdx', 'file_path': 'docs/docs/concepts/lcel.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/lcel.mdx'}, page_content='# LangChain Expression Language (LCEL)\\n\\n:::info Prerequisites\\n* [Runnable Interface](/docs/concepts/runnables)\\n:::\\n\\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\\n\\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\\n\\nWe often refer to a `Runnable` created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\\n\\n:::note\\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n:::\\n\\n## Benefits of LCEL\\n\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\\n\\nOther benefits include:\\n\\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\\n\\n## Should I use LCEL?\\n\\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\\n\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\\n\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\n\\nHere are some guidelines:\\n\\n* If you are making a single LLM call, you don\\'t need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.\\n* If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\n* If you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\n## Composition Primitives\\n\\n`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).\\n\\nMany other composition primitives (e.g., [RunnableAssign](\\nhttps://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign\\n)) can be thought of as variations of these two primitives.\\n\\n:::note\\nYou can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).\\n:::\\n\\n### RunnableSequence\\n\\n`RunnableSequence` is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\ncorresponds to the following:\\n\\n```python\\noutput1 = runnable1.invoke(some_input)\\nfinal_output = runnable2.invoke(output1)\\n```\\n\\n:::note\\n`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.\\n:::\\n\\n### RunnableParallel\\n\\n`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\n\\n```python\\nfrom langchain_core.runnables import RunnableParallel\\nchain = RunnableParallel({\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n})\\n```\\n\\nInvoking the `chain` with some input:\\n\\n```python\\nfinal_output = chain.invoke(some_input)\\n```\\n\\nWill yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n\\n```python\\n{\\n    \"key1\": runnable1.invoke(some_input),\\n    \"key2\": runnable2.invoke(some_input),\\n}\\n```\\n\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\n\\n:::note\\n`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).\\n\\n* For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.\\n* For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.\\n:::\\n\\n## Composition Syntax\\n\\nThe usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\n\\n### The `|` operator\\n\\nWe have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.\\n\\n```python\\nchain = runnable1 | runnable2\\n```\\n\\nis Equivalent to:\\n\\n```python\\nchain = RunnableSequence([runnable1, runnable2])\\n```\\n\\n### The `.pipe` method`\\n\\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\\n\\n```python\\nchain = runnable1.pipe(runnable2)\\n```\\n\\n### Coercion\\n\\nLCEL applies automatic type coercion to make it easier to compose chains.\\n\\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\\n\\nThis will make the code more verbose, but it will also make it more explicit.\\n\\n#### Dictionary to RunnableParallel\\n\\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\\n\\nFor example, the following code:\\n\\n```python\\nmapping = {\\n    \"key1\": runnable1,\\n    \"key2\": runnable2,\\n}\\n\\nchain = mapping | runnable3\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n```\\n\\n:::caution\\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nmapping.invoke(some_input)\\n```\\n:::\\n\\n#### Function to RunnableLambda\\n\\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\\n\\n```\\ndef some_func(x):\\n    return x\\n\\nchain = some_func | runnable1\\n```\\n\\nIt gets automatically converted to the following:\\n\\n```python\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n```\\n\\n:::caution\\nYou have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:\\n\\n```python\\nlambda x: x + 1.invoke(some_input)\\n```\\n:::\\n\\n## Legacy chains\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and\\n`ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\n\\nIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\\n\\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\\n'),\n",
       "  Document(metadata={'file_name': 'why_langchain.mdx', 'file_path': 'docs/docs/concepts/why_langchain.mdx', 'file_type': '.mdx', 'source': 'docs/docs/concepts/why_langchain.mdx'}, page_content='# Why LangChain?\\n\\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\\nThis page will talk about the LangChain ecosystem as a whole.\\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\\n\\n## Features\\n\\nThere are several primary needs that LangChain aims to address:\\n\\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\\n\\n2. **Orchestration:** As applications become more complex, combining multiple components and models, there\\'s [a growing need to efficiently connect these elements into control flows](https://lilianweng.github.io/posts/2023-06-23-agent/) that can [accomplish diverse tasks](https://www.sequoiacap.com/article/generative-ais-act-o1/).\\n[Orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)) is crucial for building such applications.\\n\\n3. **Observability and evaluation:** As applications become more complex, it becomes increasingly difficult to understand what is happening within them.\\nFurthermore, the pace of development can become rate-limited by the [paradox of choice](https://en.wikipedia.org/wiki/Paradox_of_choice).\\nFor example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\n[Observability](https://en.wikipedia.org/wiki/Observability) and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.\\n\\n\\n## Standardized component interfaces\\n\\nLangChain provides common interfaces for components that are central to many AI applications.\\nAs an example, all [chat models](/docs/concepts/chat_models/) implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface.\\nThis provides a standard way to interact with chat models, supporting important but often provider-specific features like [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).\\n\\n\\n### Example: chat models \\n\\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical features for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\\nThe APIs for each provider differ. \\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\\n\\n```python\\n# Tool creation\\ntools = [my_tool]\\n# Tool binding\\nmodel_with_tools = model.bind_tools(tools)\\n```\\n\\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \\nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\\nLangChain\\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\\n\\n```python\\n# Define schema\\nschema = ...\\n# Bind schema to model\\nmodel_with_structure = model.with_structured_output(schema)\\n```\\n\\n### Example: retrievers\\n\\nIn the context of [RAG](/docs/concepts/rag/) and LLM application components, LangChain\\'s [retriever](/docs/concepts/retrievers/) interface provides a standard way to connect to many different types of data services or databases (e.g., [vector stores](/docs/concepts/vectorstores) or databases).\\nThe underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the [runnable interface](/docs/concepts/runnables/), meaning they can be invoked in a common manner.\\n\\n```python\\ndocuments = my_retriever.invoke(\"What is the meaning of life?\")\\n```\\n\\n## Orchestration \\n\\nWhile standardization for individual components is useful, we\\'ve increasingly seen that developers want to *combine* components into more complex applications. \\nThis motivates the need for [orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing)).\\nThere are several common characteristics of LLM applications that this orchestration layer should support:\\n\\n* **Complex control flow:** The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).\\n* **[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/):** The application needs to maintain [short-term and / or long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n* **[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/):** The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.\\n\\nThe recommended way to orchestrate components for complex applications is [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/).\\nLangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.\\nLangGraph comes with built-in support for [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), [memory](https://langchain-ai.github.io/langgraph/concepts/memory/), and other features.\\nIt\\'s particularly well suited for building [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) or [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent/) applications. \\nImportantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph **without** using LangChain components.\\n\\n:::info[Further reading]\\n\\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\\n\\n:::\\n\\n## Observability and evaluation\\n\\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \\nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \\nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\\n\\n:::info[Further reading]\\n\\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\\n\\n:::\\n\\n## Conclusion\\n\\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\\n\\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\\n'),\n",
       "  Document(metadata={'file_name': 'tutorials.mdx', 'file_path': 'docs/docs/additional_resources/tutorials.mdx', 'file_type': '.mdx', 'source': 'docs/docs/additional_resources/tutorials.mdx'}, page_content='# 3rd Party Tutorials\\n\\n##  Tutorials\\n\\n### [LangChain v 0.1 by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae0gBSJ9T0w7cu7iJZbH3T31)\\n### [Build with Langchain - Advanced by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae06tclDATrMYY0idsTdLg9v)\\n### [LangGraph by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg)\\n### [by Greg Kamradt](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5)\\n### [by Sam Witteveen](https://www.youtube.com/playlist?list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ)\\n### [by James Briggs](https://www.youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F)\\n### [by Prompt Engineering](https://www.youtube.com/playlist?list=PLVEEucA9MYhOu89CX8H3MBZqayTbcCTMr)\\n### [by Mayo Oshin](https://www.youtube.com/@chatwithdata/search?query=langchain)\\n### [by 1 little Coder](https://www.youtube.com/playlist?list=PLpdmBGJ6ELUK-v0MK-t4wZmVEbxM5xk6L)\\n### [by BobLin (Chinese language)](https://www.youtube.com/playlist?list=PLbd7ntv6PxC3QMFQvtWfk55p-Op_syO1C)\\n### [by Total Technology Zonne](https://youtube.com/playlist?list=PLI8raxzYtfGyE02fAxiM1CPhLUuqcTLWg&si=fkAye16rQKBJVHc9)\\n\\n## Courses\\n\\n### Featured courses on Deeplearning.AI\\n\\n- [LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\\n- [LangChain Chat with Your Data](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/)\\n- [Functions, Tools and Agents with LangChain](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/)\\n- [Build LLM Apps with LangChain.js](https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/)\\n\\n### Online courses\\n\\n- [Udemy](https://www.udemy.com/courses/search/?q=langchain)\\n- [DataCamp](https://www.datacamp.com/courses/developing-llm-applications-with-langchain)\\n- [Pluralsight](https://www.pluralsight.com/search?q=langchain)\\n- [Coursera](https://www.coursera.org/search?query=langchain)\\n- [Maven](https://maven.com/courses?query=langchain)\\n- [Udacity](https://www.udacity.com/catalog/all/any-price/any-school/any-skill/any-difficulty/any-duration/any-type/relevance/page-1?searchValue=langchain)\\n- [LinkedIn Learning](https://www.linkedin.com/search/results/learning/?keywords=langchain)\\n- [edX](https://www.edx.org/search?q=langchain)\\n- [freeCodeCamp](https://www.youtube.com/@freecodecamp/search?query=langchain)\\n\\n## Short Tutorials\\n\\n- [by Nicholas Renotte](https://youtu.be/MlK6SIjcjE8)\\n- [by Patrick Loeber](https://youtu.be/LbT1yp6quS8)\\n- [by Rabbitmetrics](https://youtu.be/aywZrzNaKjs)\\n- [by Ivan Reznikov](https://medium.com/@ivanreznikov/langchain-101-course-updated-668f7b41d6cb)\\n\\n## Books and Handbooks\\n\\n- [Generative AI with LangChain](https://www.amazon.com/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/ref=sr_1_1?crid=1GMOMH0G7GLR&keywords=generative+ai+with+langchain&qid=1703247181&sprefix=%2Caps%2C298&sr=8-1) by [Ben Auffrath](https://www.amazon.com/stores/Ben-Auffarth/author/B08JQKSZ7D?ref=ap_rdr&store_ref=ap_rdr&isDramIntegrated=true&shoppingPortalEnabled=true), ©️ 2023 Packt Publishing\\n- [LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) By **James Briggs** and **Francisco Ingham**\\n- [LangChain Cheatsheet](https://pub.towardsai.net/langchain-cheatsheet-all-secrets-on-a-single-page-8be26b721cde) by **Ivan Reznikov**\\n- [Dive into Langchain (Chinese language)](https://langchain.boblin.app/)\\n\\n---------------------\\n')],\n",
       " 'bm25_documents': [Document(metadata={'source': 'docs/docs/integrations/retrievers/self_query/index.mdx', 'file_path': 'docs/docs/integrations/retrievers/self_query/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='---\\nsidebar-position: 0\\n---\\n\\n# Self-querying retrievers\\n\\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query).\\n\\nimport DocCardList from \"@theme/DocCardList\";\\n\\n<DocCardList />\\n'),\n",
       "  Document(metadata={'source': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_path': 'docs/docs/integrations/providers/vectara/index.mdx', 'file_name': 'index.mdx', 'file_type': '.mdx'}, page_content='# Vectara\\n\\n>[Vectara](https://vectara.com/) provides a Trusted Generative AI platform, allowing organizations to rapidly create a ChatGPT-like experience (an AI assistant) \\n> which is grounded in the data, documents, and knowledge that they have (technically, it is Retrieval-Augmented-Generation-as-a-service).\\n\\n**Vectara Overview:**\\n[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications.\\nVectara serverless RAG-as-a-service provides all the components of RAG behind an easy-to-use API, including:\\n1. A way to extract text from files (PDF, PPT, DOCX, etc)\\n2. ML-based chunking that provides state of the art performance.\\n3. The [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model.\\n4. Its own internal vector database where text chunks and embedding vectors are stored.\\n5. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments, including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) as well as multiple reranking options such as the [multi-lingual relevance reranker](https://www.vectara.com/blog/deep-dive-into-vectara-multilingual-reranker-v1-state-of-the-art-reranker-across-100-languages), [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/), [UDF reranker](https://www.vectara.com/blog/rag-with-user-defined-functions-based-reranking). \\n6. An LLM to for creating a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents (context), including citations.\\n\\nFor more information:\\n- [Documentation](https://docs.vectara.com/docs/)\\n- [API Playground](https://docs.vectara.com/docs/rest-api/)\\n- [Quickstart](https://docs.vectara.com/docs/quickstart)\\n\\n## Installation and Setup\\n\\nTo use `Vectara` with LangChain no special installation steps are required. \\nTo get started, [sign up](https://vectara.com/integrations/langchain) for a free Vectara trial,\\nand follow the [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \\nOnce you have these, you can provide them as arguments to the Vectara `vectorstore`, or you can set them as environment variables.\\n\\n- export `VECTARA_CUSTOMER_ID`=\"your_customer_id\"\\n- export `VECTARA_CORPUS_ID`=\"your_corpus_id\"\\n- export `VECTARA_API_KEY`=\"your-vectara-api-key\"\\n\\n## Vectara as a Vector Store\\n\\nThere exists a wrapper around the Vectara platform, allowing you to use it as a `vectorstore` in LangChain:\\n\\nTo import this vectorstore:\\n```python\\nfrom langchain_community.vectorstores import Vectara\\n```\\n\\nTo create an instance of the Vectara vectorstore:\\n```python\\nvectara = Vectara(\\n    vectara_customer_id=customer_id, \\n    vectara_corpus_id=corpus_id, \\n    vectara_api_key=api_key\\n)\\n```\\nThe `customer_id`, `corpus_id` and `api_key` are optional, and if they are not supplied will be read from \\nthe environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`, respectively.\\n\\n### Adding Texts or Files\\n\\nAfter you have the vectorstore, you can `add_texts` or `add_documents` as per the standard `VectorStore` interface, for example:\\n\\n```python\\nvectara.add_texts([\"to be or not to be\", \"that is the question\"])\\n```\\n\\nSince Vectara supports file-upload in the platform, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly. \\nWhen using this method, each file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don\\'t have to use the LangChain document loader or chunking mechanism.\\n\\nAs an example:\\n\\n```python\\nvectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\\n```\\n\\nOf course you do not have to add any data, and instead just connect to an existing Vectara corpus where data may already be indexed.\\n\\n### Querying the VectorStore\\n\\nTo query the Vectara vectorstore, you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:\\n```python\\nresults = vectara.similarity_search_with_score(\"what is LangChain?\")\\n```\\nThe results are returned as a list of relevant documents, and a relevance score of each document.\\n\\nIn this case, we used the default retrieval parameters, but you can also specify the following additional arguments in `similarity_search` or `similarity_search_with_score`:\\n- `k`: number of results to return (defaults to 5)\\n- `lambda_val`: the [lexical matching](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) factor for hybrid search (defaults to 0.025)\\n- `filter`: a [filter](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview) to apply to the results (default None)\\n- `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2.\\n- `rerank_config`: can be used to specify reranker for thr results\\n   - `reranker`: mmr, rerank_multilingual_v1 or none. Note that \"rerank_multilingual_v1\" is a Scale only feature\\n   - `rerank_k`: number of results to use for reranking\\n   - `mmr_diversity_bias`: 0 = no diversity, 1 = full diversity. This is the lambda parameter in the MMR formula and is in the range 0...1\\n\\nTo get results without the relevance score, you can simply use the \\'similarity_search\\' method:\\n```python   \\nresults = vectara.similarity_search(\"what is LangChain?\")\\n```\\n\\n## Vectara for Retrieval Augmented Generation (RAG)\\n\\nVectara provides a full RAG pipeline, including generative summarization. To use it as a complete RAG solution, you can use the `as_rag` method.\\nThere are a few additional parameters that can be specified in the `VectaraQueryConfig` object to control retrieval and summarization:\\n* k: number of results to return\\n* lambda_val: the lexical matching factor for hybrid search\\n* summary_config (optional): can be used to request an LLM summary in RAG\\n   - is_enabled: True or False\\n   - max_results: number of results to use for summary generation\\n   - response_lang: language of the response summary, in ISO 639-2 format (e.g. \\'en\\', \\'fr\\', \\'de\\', etc)\\n* rerank_config (optional): can be used to specify Vectara Reranker of the results\\n   - reranker: mmr, rerank_multilingual_v1 or none\\n   - rerank_k: number of results to use for reranking\\n   - mmr_diversity_bias: 0 = no diversity, 1 = full diversity. \\n     This is the lambda parameter in the MMR formula and is in the range 0...1\\n\\nFor example:\\n\\n```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n```\\nThen you can use the `as_rag` method to create a RAG pipeline:\\n\\n```python\\nquery_str = \"what did Biden say?\"\\n\\nrag = vectara.as_rag(config)\\nrag.invoke(query_str)[\\'answer\\']\\n```\\n\\nThe `as_rag` method returns a `VectaraRAG` object, which behaves just like any LangChain Runnable, including the `invoke` or `stream` methods.\\n\\n## Vectara Chat\\n\\nThe RAG functionality can be used to create a chatbot. For example, you can create a simple chatbot that responds to user input:\\n\\n```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n\\nquery_str = \"what did Biden say?\"\\nbot = vectara.as_chat(config)\\nbot.invoke(query_str)[\\'answer\\']\\n```\\n\\nThe main difference is the following: with `as_chat` Vectara internally tracks the chat history and conditions each response on the full chat history.\\nThere is no need to keep that history locally to LangChain, as Vectara will manage it internally.\\n\\n## Vectara as a LangChain retriever only\\n\\nIf you want to use Vectara as a retriever only, you can use the `as_retriever` method, which returns a `VectaraRetriever` object.\\n```python\\nretriever = vectara.as_retriever(config=config)\\nretriever.invoke(query_str)\\n```\\n\\nLike with as_rag, you provide a `VectaraQueryConfig` object to control the retrieval parameters.\\nIn most cases you would not enable the summary_config, but it is left as an option for backwards compatibility. \\nIf no summary is requested, the response will be a list of relevant documents, each with a relevance score.\\nIf a summary is requested, the response will be a list of relevant documents as before, plus an additional document that includes the generative summary.\\n\\n## Hallucination Detection score\\n\\nVectara created [HHEM](https://huggingface.co/vectara/hallucination_evaluation_model) - an open source model that can be used to evaluate RAG responses for factual consistency. \\nAs part of the Vectara RAG, the \"Factual Consistency Score\" (or FCS), which is an improved version of the open source HHEM is made available via the API. \\nThis is automatically included in the output of the RAG pipeline\\n\\n```python\\nsummary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\\'eng\\')\\nrerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\\nconfig = VectaraQueryConfig(k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config)\\n\\nrag = vectara.as_rag(config)\\nresp = rag.invoke(query_str)\\nprint(resp[\\'answer\\'])\\nprint(f\"Vectara FCS = {resp[\\'fcs\\']}\")\\n```\\n\\n## Example Notebooks\\n\\nFor a more detailed examples of using Vectara with LangChain, see the following example notebooks:\\n* [this notebook](/docs/integrations/vectorstores/vectara) shows how to use Vectara: with full RAG or just as a retriever.\\n* [this notebook](/docs/integrations/retrievers/self_query/vectara_self_query) shows the self-query capability with Vectara.\\n* [this notebook](/docs/integrations/providers/vectara/vectara_chat) shows how to build a chatbot with Langchain and Vectara\\n\\n'),\n",
       "  Document(metadata={'source': 'docs/docs/integrations/providers/clickup.mdx', 'file_path': 'docs/docs/integrations/providers/clickup.mdx', 'file_name': 'clickup.mdx', 'file_type': '.mdx'}, page_content='# ClickUp\\n\\n>[ClickUp](https://clickup.com/) is an all-in-one productivity platform that provides small and large teams across industries with flexible and customizable work management solutions, tools, and functions.\\n>\\n>It is a cloud-based project management solution for businesses of all sizes featuring communication and collaboration tools to help achieve organizational goals.\\n\\n## Installation and Setup\\n\\n1. Create a [ClickUp App](https://help.clickup.com/hc/en-us/articles/6303422883095-Create-your-own-app-with-the-ClickUp-API)\\n2. Follow [these steps](https://clickup.com/api/developer-portal/authentication/) to get your client_id and client_secret.\\n\\n## Toolkits\\n\\n```python\\nfrom langchain_community.agent_toolkits.clickup.toolkit import ClickupToolkit\\nfrom langchain_community.utilities.clickup import ClickupAPIWrapper\\n```\\n\\nSee a [usage example](/docs/integrations/tools/clickup).\\n\\n'),\n",
       "  Document(metadata={'source': 'docs/docs/integrations/providers/cloudflare.mdx', 'file_path': 'docs/docs/integrations/providers/cloudflare.mdx', 'file_name': 'cloudflare.mdx', 'file_type': '.mdx'}, page_content='# Cloudflare\\n\\n>[Cloudflare, Inc. (Wikipedia)](https://en.wikipedia.org/wiki/Cloudflare) is an American company that provides \\n> content delivery network services, cloud cybersecurity, DDoS mitigation, and ICANN-accredited \\n> domain registration services.\\n\\n>[Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/) allows you to run machine \\n> learning models, on the `Cloudflare` network, from your code via REST API.\\n\\n\\n## LLMs\\n\\nSee [installation instructions and usage example](/docs/integrations/llms/cloudflare_workersai).\\n\\n```python\\nfrom langchain_community.llms.cloudflare_workersai import CloudflareWorkersAI\\n```\\n\\n## Embedding models\\n\\nSee [installation instructions and usage example](/docs/integrations/text_embedding/cloudflare_workersai).\\n\\n```python\\nfrom langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\\n```\\n')]}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0323d-0c40-4ade-8f86-8bf7627c6624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a07936-2c5d-4b5c-9809-0285059db5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb7b72-edf9-4e74-b08b-d42da86304ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e9b25-e54d-4c45-ac6b-b7bf918cb687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c673765-86ea-4db8-876a-822454f774a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2297a-3feb-4105-939a-8d21f21f706c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a9d7f-1cdd-407b-b364-0e93ad36df2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991421c-05c4-44a4-b3ec-6ce0fbb4b883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

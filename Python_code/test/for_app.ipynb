{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ff46f6-f407-4411-9f59-fdba83ed7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import glob\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import torchvision\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f727db-8ff7-4bd7-91cb-bba883d20c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 640, 640)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_size = 640\n",
    "path = glob.glob('*.jpg')\n",
    "# img_list = []\n",
    "# for img in path:\n",
    "#     image = Image.open(path[0])\n",
    "#     image.thumbnail((target_size, target_size))\n",
    "#     padded_img = ImageOps.pad(image, (target_size, target_size))\n",
    "#     img_np = np.array(padded_img).astype(np.float32)/255.0\n",
    "#     img_np = img_np.transpose([2, 0, 1])\n",
    "#     img_np = np.expand_dims(img_np, axis=0)\n",
    "#     img_list.append(img_np)\n",
    "\n",
    "# np.array(img_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70827dce-4386-4168-ae3c-ea47508ebffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession('best.onnx')\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "result = session.run([output_name], {input_name: img_list[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb451b1-af77-4a98-ba4c-c38d1a4c8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像 0: 検出されたオブジェクトの数: 4\n",
      "    座標: [tensor(483.90857), tensor(543.44012), tensor(529.23657), tensor(598.27289)], 信頼度: 0.9582321643829346, クラス: 0.0\n",
      "    座標: [tensor(389.48328), tensor(70.23535), tensor(432.98383), tensor(127.41638)], 信頼度: 0.9482046961784363, クラス: 0.0\n",
      "    座標: [tensor(431.67334), tensor(69.08841), tensor(474.15851), tensor(127.48247)], 信頼度: 0.9455069899559021, クラス: 1.0\n",
      "    座標: [tensor(473.43066), tensor(67.75636), tensor(515.72369), tensor(125.53108)], 信頼度: 0.9404840469360352, クラス: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "yolov5.githubの*.pyの関数を使用した場合\n",
    "\"\"\"\n",
    "# import torch\n",
    "# from utils.general import non_max_suppression\n",
    "\n",
    "# # モデルからの出力\n",
    "# predictions = result[0]  # ここで result は session.run からの出力\n",
    "\n",
    "# # NumPy配列からPyTorchテンソルへの変換\n",
    "# predictions_tensor = torch.from_numpy(predictions).float() \n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# predictions_tensor = predictions_tensor.to(device)\n",
    "\n",
    "# # 非最大抑制(NMS)を適用\n",
    "# conf_thres = 0.6  # 信頼度閾値\n",
    "# iou_thres = 0.45  # IOU閾値\n",
    "# classes = None  # 特定のクラスにフィルタリングする場合は、クラスIDのリスト\n",
    "# agnostic_nms = False  # クラスに依存しないNMSを使用するかどうか\n",
    "# max_det = 100  # 画像あたりの最大検出数\n",
    "\n",
    "# # NMS処理\n",
    "# nms_predictions = non_max_suppression(predictions_tensor, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms, max_det=max_det)\n",
    "\n",
    "# # 処理後の検出を処理\n",
    "# for i, det in enumerate(nms_predictions):  # 画像ごとの検出結果をループ\n",
    "#     if len(det):\n",
    "#         # det[:, :4] はバウンディングボックスの座標(x1, y1, x2, y2)\n",
    "#         # det[:, 4] はオブジェクトネススコア\n",
    "#         # det[:, 5] は最も可能性の高いクラスID\n",
    "#         print(f\"画像 {i}: 検出されたオブジェクトの数: {len(det)}\")\n",
    "#         for *xyxy, conf, cls in det:\n",
    "#             print(f\"    座標: {xyxy}, 信頼度: {conf}, クラス: {cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f173a-854d-47e8-8e20-066bcc28013d",
   "metadata": {},
   "source": [
    "## スクラッチで実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "060adb38-ffc2-4bb5-bce2-68a76a944f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------\n",
    "# \n",
    "#------------\n",
    "\n",
    "#runの引数\n",
    "input_file = path[0]\n",
    "#session\n",
    "\n",
    "#--------------\n",
    "\n",
    "# run()\n",
    "max_det = 1000  # 画像あたりの最大bbox数\n",
    "conf_thres = 0.6  # 信頼値の閾値\n",
    "iou_thres = 0.45  # NMS IOU の閾値\n",
    "imgsz = (640, 640) # インプットの画像サイズ\n",
    "agnostic_nms = False # 異なるbboxの重なりをマージするか\n",
    "hide_conf = False # 信頼度スコアの非表示をするか\n",
    "stride = 32 \n",
    "\n",
    "# onnxに保存されているメタデータ -> {'names': \"{0: '押印', 1: '未押印'}\", 'stride': '32'}\n",
    "meta = session.get_modelmeta().custom_metadata_map\n",
    "\n",
    "# metadataがあればをアンパックする\n",
    "if 'stride' in meta:\n",
    "    stride, names = int(meta['stride']), eval(meta['names'])\n",
    "\n",
    "# 16ビット浮動小数点精度の使用\n",
    "fp16 = False\n",
    "\n",
    "# 画像のサイズをstrideの倍数にする\n",
    "imgsz = [max(math.ceil(x / stride) * stride, 0) for x in imgsz]\n",
    "output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bee9da50-5c44-4704-9c65-855c141edce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 画像の前処理 ----\n",
    "np_img = cv2.imread(input_file)\n",
    "\n",
    "# 画像のリサイズ\n",
    "img = letterbox(np_img, [640, 640])[0]\n",
    "\n",
    "# (H,W,C) -> (C, H, W)\n",
    "# BGR -> RGB\n",
    "img = img.transpose((2, 0, 1))[::-1]\n",
    "img = np.ascontiguousarray(img)\n",
    "\n",
    "# テンソル化 -> 正規化\n",
    "img = torch.from_numpy(img)\n",
    "img = img.half() if fp16 else img.float()\n",
    "img /= 255\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "#---- 推論 -----\n",
    "img = img.cpu().numpy()\n",
    "output_name = session.get_outputs()[0].name\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# yのshape:[データ数、bbox数, メタ情報]\n",
    "# メタ情報:[x中心, y中心, bboxの幅, bboxの高さ, 信頼スコア、 押印の確率, 未押印の確率]\n",
    "y = session.run([output_name], {input_name: img})[0]\n",
    "pred = torch.from_numpy(y) # テンソル化\n",
    "\n",
    "# NMS処理\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic_nms, max_det=max_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "66e62e24-fe64-4f2b-8257-0541f29f001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[483.87460, 543.45117, 529.17871, 598.46057,   0.95847,   0.00000],\n",
       "         [389.51276,  70.15652, 432.96857, 127.43932,   0.94714,   0.00000],\n",
       "         [431.27887,  69.81499, 474.19971, 127.03190,   0.94670,   1.00000],\n",
       "         [473.34045,  67.55436, 515.70135, 125.63311,   0.94446,   1.00000]])]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1345452-d754-4fea-916e-edb3dd57ad2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528c199-19e0-401b-a24c-1f93749531e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23592c-85b5-4dc4-8c02-dc2613f4960d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfc1e4-0f98-4065-b70a-9698f04b2ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9acd9d-2458-49ff-ba41-d399836d239b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6d8db-ee47-4c6f-9a6a-d31d44a87e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153cf67-fb2a-46d7-b0d2-c068490da408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1c3ec-614c-4ecc-8c54-e96d06800c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a1aa6-14c5-49ff-aa67-aa0c29d67f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf02ee6-492c-41e2-8d2d-eb4297606a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a9801-f6b6-455e-840c-d6007cb3b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f692e02-f3b5-4d4c-ad88-186a5b02c942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa27a0-a16f-4c25-9423-f787683b4ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633bda2-6840-4b4f-968c-e2c1b2b73aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a99e18-3c46-4503-9392-01c39037218b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afc6db-cdfe-4ac6-8863-3d61abac67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea656e2-f0bf-436d-a401-e82f32c601c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edf501-ab1d-46c6-9a13-6a22774bb6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68cef0-be91-455c-a778-eb70a7868709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841003bd-9214-4061-88e4-6b4771deed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a145b59-03e7-4765-84b8-93fce76ecfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15fc81-2cb3-4fdf-b85b-d125045873a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85442543-4169-40ee-9793-49c0a9461985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f6502c-6656-41b4-90c5-1d482d7f0a15",
   "metadata": {},
   "source": [
    "## non_max_suppression関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "317a8e7d-86c1-4dc0-8c8d-69fe1b25d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "    prediction, \n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    agnostic=False,\n",
    "    labels=(),\n",
    "    max_det=300,\n",
    "    nm=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    YOLOの予測結果をNMS処理する。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    prediction : (tensor) テンソル化したyoloが予測した結果\n",
    "    conf_thres : (float) バウンディングボックス(bbox)の信頼度スコアの閾値\n",
    "    iou_thres : (float) IOUの閾値\n",
    "    agnostic : (Bool) 重なった異なるクラスのbboxを同一のbboxにするか\n",
    "    labels : (tuple or list-like) 画像内のラベルの情報\n",
    "    max_det : (int) 最大のbbox数\n",
    "    nm : (int) マスク数\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (tensor) NMS処理後のbbox情報を含むテンソル、[x1, y1, x2, y2, クラス確率, cls_id]のカラムに変換されている\n",
    "                      x1,y1はbboxの左上のx,y座標、x2, y2は右下のx,y座標を表す\n",
    "                      \n",
    "    \"\"\"\n",
    "    batch_size = prediction.shape[0] # バッチサイズ\n",
    "    num_class = prediction.shape[2] - nm - 5 # クラス数\n",
    "    bool_cnf = prediction[..., 4] > conf_thres # bbox毎に信頼度スコアがconf_thresより大きいかのbool\n",
    "    \n",
    "    max_wh = 7680  # 最大のbboxの幅、高さ\n",
    "    max_nms = 30000  # torchvision.ops.nms()のための最大のbbox数\n",
    "    time_limit = 0.5 + 0.05 * batch_size  # タイムリミット(s)\n",
    "    \n",
    "    start = time.time()\n",
    "    mi = 5 + num_class\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * batch_size\n",
    "    \n",
    "    \n",
    "    for idx, x in enumerate(prediction):\n",
    "    \n",
    "        # conf_thresより大きい信頼度スコアを持つbboxsを抽出\n",
    "        x = x[bool_cnf[idx]]\n",
    "    \n",
    "        # labelsを持っていた時の処理\n",
    "        if labels and len(labels[idx]):\n",
    "                lb = labels[idx]\n",
    "                v = torch.zeros((len(lb), num_class + nm + 5), device=x.device)\n",
    "                v[:, :4] = lb[:, 1:5]  # box\n",
    "                v[:, 4] = 1.0  # conf\n",
    "                v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "                x = torch.cat((x, v), 0)\n",
    "            \n",
    "    \n",
    "        # フィルター後のbboxがない場合\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "            \n",
    "        x[:, 5:] *= x[:, 4:5] # 信頼度スコア*各cls確率\n",
    "        box = xywh2xyxy(x[:, :4]) # [x1, y1, x2, y2]:bboxの左上と右下の座標に変換\n",
    "        mask = x[:, mi:] # maskがなければ[]\n",
    "    \n",
    "        cls_prob, cls_id = x[:, 5:mi].max(1, keepdim=True) # 各bboxで確率の高いclsの値とcls_idを返す\n",
    "    \n",
    "        # [x1, y1, x2, y2, clsの確率, cls_id]のカラムの順に結合\n",
    "        # clsの確率(本来のprob*信頼度スコア)がconf_thresより高いもののみ抽出\n",
    "        x = torch.cat((box, cls_prob, cls_id.float(), mask), axis=1)[cls_prob.view(-1) > conf_thres]\n",
    "    \n",
    "        \n",
    "        n = x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "    \n",
    "        # cls確率をキーにして降順に並び替え、且つ max_nmsを超えないようにする\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]] \n",
    "    \n",
    "        # 異なるクラスのbboxを区別して評価するため、agnosticでクラス固有のbboxを作成\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "    \n",
    "        # nms\n",
    "        selected_idx = torchvision.ops.nms(boxes, scores, iou_thres)\n",
    "        selected_idx = selected_idx[:max_det]\n",
    "\n",
    "        output[idx] = x[selected_idx]\n",
    "        finish = time.time()\n",
    "        if (finish - start) > time_limit:\n",
    "            break\n",
    "            \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5142b-a5ee-46b0-8600-09d2c9cd217e",
   "metadata": {},
   "source": [
    "## 'xywh2xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1cea98a0-b504-4f55-8835-a254df73cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    [x中心、y中心、bboxの幅, bboxの高さ] -> [bboxの左上のx、bboxの左上のy, bboxの右下のx, bboxの右下のy]\n",
    "    \n",
    "    \"\"\"\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # bboxの左上のx\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # bboxの左上のy\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bboxの右下のx\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bboxの右下のy\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1bbed-ec26-43b0-aabe-73e11eaa8974",
   "metadata": {},
   "source": [
    "# letterbox関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1935e7b1-c605-4813-a7ae-1141fcb7c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(\n",
    "    img,\n",
    "    new_shape=(640, 640),\n",
    "    color=(0, 0, 0),\n",
    "    scaleup=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    指定されたサイズに画像をリサイズを行う。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    img : (ndarray) 画像のndarray配列\n",
    "    new_shape : (tuple) リサイズしたい画像サイズ\n",
    "    color : (tuple:(B, G, R)) パディングの色\n",
    "    scaleup : (Bool) スケールアップを行う場合はTrue\n",
    "\n",
    "    Return\n",
    "    -----------\n",
    "    padded_img : (ndarray) リサイズされた画像のndarray配列\n",
    "    ratio : (tuple リサイズした比率\n",
    "    (dw, dh) : (int:(横、高さ)) パディングした数値(横、高さ)\n",
    "    \"\"\"\n",
    "    \n",
    "    # アスペクト比を保った画像のリサイズ\n",
    "    ori_shape = img.shape[:2] # [H, W, C] -> [H, W]\n",
    "    r = min(new_shape[0] / ori_shape[0], new_shape[1] / ori_shape[1])\n",
    "    \n",
    "    # スケールアップなしの場合\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "    \n",
    "    # アスペクト比を保ったまま、リスケール\n",
    "    ratio = (r, r)\n",
    "    new_unpad = (round(ori_shape[1] * r), round(ori_shape[0] * r)) # cv.resizeのため[W, H]にする\n",
    "    \n",
    "    # パディングする領域を算出\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw, dh = dw / 2, dh / 2\n",
    "\n",
    "    # dw, dfが.5の時の対応 -> この微調整をしないとsession.run()時のinputのshapeが合わなくなる\n",
    "    top, bottom = round(dh - 0.1), round(dh + 0.1)\n",
    "    left, right = round(dw - 0.1), round(dw + 0.1)\n",
    "    \n",
    "    # 画像のリサイズ\n",
    "    if new_shape[::-1] != new_unpad:\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "    return padded_img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316199d-9e82-4cb3-8c67-c08eb0d3cc03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

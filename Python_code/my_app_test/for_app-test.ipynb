{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ff46f6-f407-4411-9f59-fdba83ed7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import glob\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import torchvision\n",
    "import math\n",
    "from  io import BytesIO\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70827dce-4386-4168-ae3c-ea47508ebffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob.glob('*.jpg')\n",
    "session = ort.InferenceSession('best.onnx')\n",
    "# input_name = session.get_inputs()[0].name\n",
    "# output_name = session.get_outputs()[0].name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f173a-854d-47e8-8e20-066bcc28013d",
   "metadata": {},
   "source": [
    "## スクラッチで実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6328c799-aabb-4e11-a274-99731cf67654",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = path[0]\n",
    "\n",
    "\n",
    "max_det = 1000  # 画像あたりの最大bbox数\n",
    "conf_thres = 0.6  # 信頼値の閾値\n",
    "iou_thres = 0.45  # NMS IOU の閾値\n",
    "imgsz = (640, 640) # インプットの画像サイズ\n",
    "agnostic_nms = False # 異なるbboxの重なりをマージするか\n",
    "hide_conf = False # 信頼度スコアの非表示をするか\n",
    "stride = 32 \n",
    "\n",
    "# onnxに保存されているメタデータ -> {'names': \"{0: '押印', 1: '未押印'}\", 'stride': '32'}\n",
    "meta = session.get_modelmeta().custom_metadata_map\n",
    "\n",
    "# metadataがあればをアンパックする\n",
    "if 'stride' in meta:\n",
    "    stride, names = int(meta['stride']), eval(meta['names'])\n",
    "\n",
    "# 16ビット浮動小数点精度の使用\n",
    "fp16 = False\n",
    "\n",
    "# 画像のサイズをstrideの倍数にする\n",
    "imgsz = [max(math.ceil(x / stride) * stride, 0) for x in imgsz]\n",
    "\n",
    "\n",
    "#==============\n",
    "# 画像の前処理\n",
    "#==============\n",
    "\n",
    "np_img = cv2.imread(input_file)\n",
    "\n",
    "# 画像のリサイズ\n",
    "img = letterbox(np_img, [640, 640])[0]\n",
    "\n",
    "# (H,W,C) -> (C, H, W)\n",
    "# BGR -> RGB\n",
    "img = img.transpose((2, 0, 1))[::-1]\n",
    "img = np.ascontiguousarray(img)\n",
    "\n",
    "# テンソル化 -> 正規化\n",
    "img = torch.from_numpy(img)\n",
    "img = img.half() if fp16 else img.float()\n",
    "scaled_img = img / 255\n",
    "scaled_img = scaled_img.unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d91fdc5-1499-4bf8-bc4c-b8646f24b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[459.0970,  63.0555, 507.4164, 120.4762,   0.9282,   0.0000],\n",
      "        [459.0384,  61.7044, 506.8535, 121.1854,   0.8347,   1.0000]])]\n",
      "classes: ['押印', '未押印']\n",
      "cls_probs: [0.928, 0.835]\n",
      "return (1, output_img)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============\n",
    "# 推論 \n",
    "# ===============\n",
    "\n",
    "scaled_img = scaled_img.cpu().numpy()\n",
    "output_name = session.get_outputs()[0].name\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# yのshape:(データ数、bbox数, メタ情報)\n",
    "# メタ情報:[x中心, y中心, bboxの幅, bboxの高さ, 信頼スコア、 押印の確率, 未押印の確率]\n",
    "y = session.run([output_name], {input_name: scaled_img})[0]\n",
    "pred = torch.from_numpy(y) # テンソル化\n",
    "\n",
    "# NMS処理\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic_nms, max_det=max_det)\n",
    "\n",
    "print(pred)\n",
    "\n",
    "# ===============\n",
    "# 出力処理 \n",
    "# ==============\n",
    "pred_cls_list = pred[0][:, -1] # 全bboxのクラス識別結果\n",
    "no_seal_cls_id = list(eval(meta['names']))[1] # 未押印のcls_id\n",
    "\n",
    "\n",
    "# 未押印の識別があったら、画像出力の分岐\n",
    "if no_seal_cls_id in pred_cls_list:\n",
    "\n",
    "    # --- 未押印あり -> bboxつきの画像配列を渡す処理 -------------\n",
    "    for det in pred:\n",
    "        \n",
    "        process_img = np_img.copy() # np化したオリジナルimg\n",
    "        \n",
    "        if len(det):\n",
    "            # bboxの情報をオリジナル画像用に調整\n",
    "            det[:, :4] = scale_boxes(scaled_img.shape[2:], det[:, :4], process_img.shape).round()\n",
    "    \n",
    "            # bbox毎にイテレート\n",
    "            for *xyxy, prob, cls_id in reversed(det):\n",
    "                cls_id = int(cls_id)\n",
    "    \n",
    "                # bboxの色 (cv2に合わせるため、BGR)、YOLOv5に合わせている\n",
    "                # class_id:[0, 1]の2クラス用 -> いずれclassにしたら、インスタンス変数にしたい\n",
    "                color = [(56, 56, 255), (151, 157, 255)][cls_id]\n",
    "                label = names if hide_conf else f\"{names[cls_id]} {prob:.2f}\"\n",
    "    \n",
    "                # 最後にPILで処理されて,ChannelがRGBに変わっているので注意!!\n",
    "                # 順々に検出したbboxの枠を作成しるのでinputのimg変数と格納する変数名を同じにする。\n",
    "                process_img = box_label(img = process_img, box = xyxy, label = label, color = color)\n",
    "    \n",
    "            # POSTのResponseのため、RGBに変換後、(C, H, W)に転置する。\n",
    "            output_img = cv2.cvtColor(process_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # 各クラス情報と確率のリスト\n",
    "            cls_info = det[:, -2:].data.numpy().astype(np.float64)\n",
    "            classes = [ names[cls] for cls in cls_info[:, -1]]\n",
    "            cls_probs = list(cls_info[:, 0].round(decimals=3))\n",
    "            print('classes:', classes)\n",
    "            print('cls_probs:', cls_probs)\n",
    "\n",
    "    # 1とbbox付き画像のndarray配列を返す予定\n",
    "    # return後、ファイル名の取得も忘れずにする（おそらくforで回しているので）!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    print('return (1, output_img)')\n",
    "\n",
    "else:\n",
    "    # --- 未押印なし -------------\n",
    "    \n",
    "    # 1と出力する画像はないのでNoneを返す予定\n",
    "    print('return (0, None)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165425a2-6a90-49d3-9cf7-190816b7d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d4dc6c91-4dbf-4908-9a32-2c3bcad6637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = det[:, -2:].data.numpy()\n",
    "# np.where(a[1]==0, names[0], names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a23f2ab3-1413-4eeb-ab43-da1e1ea95026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['押印', '未押印']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[names[cls] for cls in a[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e79f5e0a-9c7b-4819-bd60-c2aee6bfc2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92817795, 0.83474398])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  cls_info[:, 0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0196e61-e164-4226-b893-618eb3ae8c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type list doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcls_probs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: type list doesn't define __round__ method"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70f31b-3b11-42b9-9769-09e3de43af8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf64e88d-b24d-40ab-8d80-ec17319d98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のbyte化\n",
    "\n",
    "cls_id, no_seal_img = run(session, input_file)\n",
    "\n",
    "img = Image.fromarray(no_seal_img)\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format='JPEG')\n",
    "img_byte = buffered.getvalue()\n",
    "img_base64 = base64.b64encode(img_byte)\n",
    "img_str = img_base64.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec3809bd-c84a-49c1-870b-2dd1ab72670f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_str[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19e41d-a577-424a-ae53-137397362bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087960e2-3fab-4c83-894b-ada75b1ee95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072226c5-6dd7-4d8d-9f60-3d1669a016fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f5e9017-a083-4fe6-8606-c8fede80f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後のresponseでの判定 -> noseal_count >= 0 -> \n",
    "\n",
    "a = 'img.jpg'\n",
    "b = '押印あり'\n",
    "c = 0\n",
    "d = [a, b, c]\n",
    "\n",
    "e = 'badimg.jpg'\n",
    "f = '押印なし'\n",
    "g = 1\n",
    "\n",
    "h = [e, f, g]\n",
    "\n",
    "test_list = [d, h, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31d67002-51a3-4498-a8e2-1a07ef1c0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_seal_count = 0\n",
    "for img_output in test_list:\n",
    "    no_seal_count += img_output[2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79590474-8769-444f-877c-dc1654aaa2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8cc92-ce7b-468f-8d1a-1df9b64f38de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba320ba-46bc-44c3-bc66-0a1674edecff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eac2f3-8e0c-43c9-801d-4a4e2437c76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4cc4a5b-df5a-45a1-861c-12947b3d70bb",
   "metadata": {},
   "source": [
    "## box_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c68d265-f98e-458c-8966-db9c60cf20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_label (\n",
    "    img,\n",
    "    box,\n",
    "    label =\"\",\n",
    "    color = (56, 56, 255),\n",
    "    txt_color = (255, 255, 255),\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    オリジナル画像にクラス、クラス確率のテキストが付与されたbboxを描画する。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    img : (ndarray) cv2処理されたオリジナル画像のndarray配列。\n",
    "                    Channelは(B, G, R)。\n",
    "    box : (tensor) リスケール画像のbboxの情報　[x1, y1, x2, y2]。\n",
    "                   x1,y1はbboxの左上のx,y座標、x2, y2は右下のx,y座標を表す。\n",
    "    label : (str) bboxに付与されるテキスト \"bboxのクラス名 クラス確率(下2桁)\"の文字列。\n",
    "    color : (tuple | list-like) bboxの枠の色 (B, G, R)。\n",
    "    txt_color : (tuple | list-like) bboxの付与するテキストの色。 \n",
    "                                    PIL処理のため、Channelは(R, G, B)。\n",
    "\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (ndarray) オリジナル画像にbbox(テキスト付き)を描画したndarray配列。\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # YOLOv5の使用より (参照：ultralytics/ultralytics/utils/plotting.py)\n",
    "    # line_widthは　1　と　2で挙動を確認済み\n",
    "    line_width = max(math.floor(sum(img.shape) / 2 * 0.003), 2) # bboxの枠用に調整\n",
    "    \n",
    "    # p1:(x1, y1), p2:(x2, y2)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    \n",
    "    # bboxの描画\n",
    "    cv2.rectangle(img, p1, p2, color, thickness=line_width, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    if label:\n",
    "        \n",
    "        # 日本語フォントファイルのパス\n",
    "        font_path = \"font/Koruri-Bold.ttf\"\n",
    "    \n",
    "        # 日本語フォントを読み込み\n",
    "        font_size = int(line_width * 6)\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "        # テキストのサイズを計算\n",
    "        text_bbox = font.getbbox(label) # return -> bboxの(left, top, right, bottom) \n",
    "        w, h = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "        # テキスト用の領域描画\n",
    "        outside = p1[1] - h >= 3\n",
    "        p2 = p1[0] + w + 7, p1[1] - h - 5 if outside else p1[1] + h + 5 # テキスト分の右上座標を計算 \n",
    "        cv2.rectangle(img, p1, p2, color, -1, cv2.LINE_AA)  # テキスト領域の塗り潰し\n",
    "    \n",
    "        #------------ PIL (BGR -> RGB)　-------------\n",
    "        # PILイメージに変換\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGB変換\n",
    "        img_pil = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "    \n",
    "        # テキストを描画\n",
    "        position = (p1[0]+ 4, p2[1] if outside else p1[1] + h + 2)\n",
    "        draw.text(position, label, font=font, fill=txt_color)\n",
    "    \n",
    "        # ndarrayに変換\n",
    "        img = np.array(img_pil)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGRに戻す\n",
    "        \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afc6db-cdfe-4ac6-8863-3d61abac67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06f13065-fb73-4cd3-93bf-e0921ea12242",
   "metadata": {},
   "source": [
    "## scale_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4edf501-ab1d-46c6-9a13-6a22774bb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_boxes(scaled_img_shape, boxes, img_shape):\n",
    "    \"\"\"\n",
    "    渡されたbboxをオリジナル画像用に調整する。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    scaled_img_shape : (tuple) リスケールした画像のshape()\n",
    "    boxes : (tensor) リスケール画像用bboxの情報　[x1, y1, x2, y2]\n",
    "                     x1,y1はbboxの左上のx,y座標、x2, y2は右下のx,y座標を表す\n",
    "    img_shape : (tuple) オリジナル画像のshape, (H, W, C)の順番\n",
    "\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (tensor) オリジナル画像用に調整したbboxの情報 [x1, y1, x2, y2]\n",
    "                      オリジナル画像からはみ出る大きさのbboxはクリッピングされる。\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ratio = min(scaled_img_shape[0] / img_shape[0], scaled_img_shape[1] / img_shape[1]) # new / old\n",
    "    pad = (scaled_img_shape[1] - img_shape[1] * ratio) / 2, (scaled_img_shape[0] - img_shape[0] * ratio) / 2 # 片面のpad計算 \n",
    "    \n",
    "    # 元画像用にbboxを調整するため,\n",
    "    # 片方のpad分を引き、リスケールの比率で割る\n",
    "    boxes[:, [0, 2]] -= pad[0] # x padding\n",
    "    boxes[:, [1, 3]] -= pad[1] # y padding\n",
    "    boxes /= ratio \n",
    "    \n",
    "    # 0以上、オリジナル画像の幅、高さ以内にbboxが入るようにクリップ\n",
    "    clip_boxes(boxes, img_shape)\n",
    "\n",
    "    return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc0822-acc1-4266-a992-20a2fd5b7ca1",
   "metadata": {},
   "source": [
    "## clip_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63fbe51-5cf2-404b-8e0a-bb956d4536de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    bboxがオリジナル画像の範囲内に収まるようにクリッピング\n",
    "    0以上もしくはオリジナル画像の幅、高さに制限する。\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    boxes : (tensor) オリジナル画像用に調整したbbox, [x1, y1, x2, y2],\n",
    "                     x1,y1はbboxの左上のx,y座標、x2, y2は右下のx,y座標を表す\n",
    "    shape : (tuple) オリジナル画像のshape, (H, W, C)の順番\n",
    "\n",
    "\n",
    "    Return\n",
    "    -----------\n",
    "    output : None \n",
    "             しかし、クリッピングにより変更がある場合は、直接boxesに直接変更が加えられる。\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # bboxがオリジナル画像の範囲内に収まるようにクリッピング\n",
    "    boxes[..., 0].clamp_(0, shape[1])  # x1\n",
    "    boxes[..., 1].clamp_(0, shape[0])  # y1\n",
    "    boxes[..., 2].clamp_(0, shape[1])  # x2\n",
    "    boxes[..., 3].clamp_(0, shape[0])  # y2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6502c-6656-41b4-90c5-1d482d7f0a15",
   "metadata": {},
   "source": [
    "## non_max_suppression関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317a8e7d-86c1-4dc0-8c8d-69fe1b25d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "    prediction, \n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    agnostic=False,\n",
    "    labels=(),\n",
    "    max_det=300,\n",
    "    nm=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    YOLOの予測結果をNMS処理する。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    prediction : (tensor) テンソル化したyoloが予測した結果\n",
    "    conf_thres : (float) バウンディングボックス(bbox)の信頼度スコアの閾値\n",
    "    iou_thres : (float) IOUの閾値\n",
    "    agnostic : (Bool) 重なった異なるクラスのbboxを同一のbboxにするか\n",
    "    labels : (tuple or list-like) 画像内のラベルの情報\n",
    "    max_det : (int) 最大のbbox数\n",
    "    nm : (int) マスク数\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (tensor) NMS処理後のbbox情報を含むテンソル、[x1, y1, x2, y2, クラス確率, cls_id]のカラムに変換されている\n",
    "                      x1,y1はbboxの左上のx,y座標、x2, y2は右下のx,y座標を表す\n",
    "                      \n",
    "    \"\"\"\n",
    "    batch_size = prediction.shape[0] # バッチサイズ\n",
    "    num_class = prediction.shape[2] - nm - 5 # クラス数\n",
    "    bool_cnf = prediction[..., 4] > conf_thres # bbox毎に信頼度スコアがconf_thresより大きいかのbool\n",
    "    \n",
    "    max_wh = 7680  # 最大のbboxの幅、高さ\n",
    "    max_nms = 30000  # torchvision.ops.nms()のための最大のbbox数\n",
    "    time_limit = 0.5 + 0.05 * batch_size  # タイムリミット(s)\n",
    "    \n",
    "    start = time.time()\n",
    "    mi = 5 + num_class\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * batch_size\n",
    "    \n",
    "    \n",
    "    for idx, x in enumerate(prediction):\n",
    "    \n",
    "        # conf_thresより大きい信頼度スコアを持つbboxsを抽出\n",
    "        x = x[bool_cnf[idx]]\n",
    "    \n",
    "        # labelsを持っていた時の処理\n",
    "        if labels and len(labels[idx]):\n",
    "                lb = labels[idx]\n",
    "                v = torch.zeros((len(lb), num_class + nm + 5), device=x.device)\n",
    "                v[:, :4] = lb[:, 1:5]  # box\n",
    "                v[:, 4] = 1.0  # conf\n",
    "                v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "                x = torch.cat((x, v), 0)\n",
    "            \n",
    "    \n",
    "        # フィルター後のbboxがない場合\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "            \n",
    "        x[:, 5:] *= x[:, 4:5] # 信頼度スコア*各cls確率\n",
    "        box = xywh2xyxy(x[:, :4]) # [x1, y1, x2, y2]:bboxの左上と右下の座標に変換\n",
    "        mask = x[:, mi:] # maskがなければ[]\n",
    "    \n",
    "        cls_prob, cls_id = x[:, 5:mi].max(1, keepdim=True) # 各bboxで確率の高いclsの値とcls_idを返す\n",
    "    \n",
    "        # [x1, y1, x2, y2, clsの確率, cls_id]のカラムの順に結合\n",
    "        # clsの確率(本来のprob*信頼度スコア)がconf_thresより高いもののみ抽出\n",
    "        x = torch.cat((box, cls_prob, cls_id.float(), mask), axis=1)[cls_prob.view(-1) > conf_thres]\n",
    "    \n",
    "        \n",
    "        n = x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "    \n",
    "        # cls確率をキーにして降順に並び替え、且つ max_nmsを超えないようにする\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]] \n",
    "    \n",
    "        # 異なるクラスのbboxを区別して評価するため、agnosticでクラス固有のbboxを作成\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "    \n",
    "        # nms\n",
    "        selected_idx = torchvision.ops.nms(boxes, scores, iou_thres)\n",
    "        selected_idx = selected_idx[:max_det]\n",
    "\n",
    "        output[idx] = x[selected_idx]\n",
    "        finish = time.time()\n",
    "        if (finish - start) > time_limit:\n",
    "            break\n",
    "            \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5142b-a5ee-46b0-8600-09d2c9cd217e",
   "metadata": {},
   "source": [
    "## 'xywh2xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cea98a0-b504-4f55-8835-a254df73cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    [x中心、y中心、bboxの幅, bboxの高さ] -> [bboxの左上のx、bboxの左上のy, bboxの右下のx, bboxの右下のy]\n",
    "    \n",
    "    \"\"\"\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # bboxの左上のx\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # bboxの左上のy\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bboxの右下のx\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bboxの右下のy\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1bbed-ec26-43b0-aabe-73e11eaa8974",
   "metadata": {},
   "source": [
    "# letterbox関数の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1935e7b1-c605-4813-a7ae-1141fcb7c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(\n",
    "    img,\n",
    "    new_shape=(640, 640),\n",
    "    color=(114, 114, 114),\n",
    "    scaleup=True,\n",
    "    stride = 32\n",
    "):\n",
    "    \"\"\"\n",
    "    指定されたサイズに画像をリサイズを行う。\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    img : (ndarray) 画像のndarray配列\n",
    "    new_shape : (tuple) リサイズしたい画像サイズ\n",
    "    color : (tuple:(B, G, R)) パディングの色\n",
    "    scaleup : (Bool) スケールアップを行う場合はTrue\n",
    "\n",
    "    Return\n",
    "    -----------\n",
    "    padded_img : (ndarray) リサイズされた画像のndarray配列\n",
    "    ratio : (tuple リサイズした比率\n",
    "    (dw, dh) : (int:(横、高さ)) パディングした数値(横、高さ)\n",
    "    \"\"\"\n",
    "    \n",
    "    # アスペクト比を保った画像のリサイズ\n",
    "    ori_shape = img.shape[:2] # [H, W, C] -> [H, W]\n",
    "    r = min(new_shape[0] / ori_shape[0], new_shape[1] / ori_shape[1])\n",
    "    \n",
    "    # スケールアップなしの場合\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "    \n",
    "    # アスペクト比を保ったまま、リスケール\n",
    "    ratio = (r, r)\n",
    "    new_unpad = (round(ori_shape[1] * r), round(ori_shape[0] * r)) # cv.resizeのため[W, H]にする\n",
    "    \n",
    "    # パディングする領域を算出\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw, dh = dw / 2, dh / 2\n",
    "\n",
    "    # dw, dfが.5の時の対応 -> この微調整をしないとsession.run()時のinputのshapeが合わなくなる\n",
    "    top, bottom = round(dh - 0.1), round(dh + 0.1)\n",
    "    left, right = round(dw - 0.1), round(dw + 0.1)\n",
    "    \n",
    "    # 画像のリサイズ\n",
    "    if new_shape[::-1] != new_unpad:\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "    return padded_img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316199d-9e82-4cb3-8c67-c08eb0d3cc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006857ec-4c78-4eee-b755-e69a06f8745e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b8fbc-db4d-4d1b-807e-0d512c43c8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

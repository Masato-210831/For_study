{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c9a777-0600-4563-97cb-dad93df8d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import glob\n",
    "from PIL import Image, ImageOps\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd3b4d-eb24-4ca5-8212-286245c3ac3b",
   "metadata": {},
   "source": [
    "## Ëá™‰ΩúÂâçÂá¶ÁêÜ„ÄÄÔºã„ÄÄonnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd0d118-37a1-41dd-b7a0-87697196dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-3-10 Python-3.8.18 torch-2.2.1+cu121 CPU\n",
      "\n",
      "Loading best.onnx for ONNX Runtime inference...\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('./hidden_others/yolov5', 'custom', 'best.onnx', source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9109321-13a2-49f8-8464-9723d5d44bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = ['test_002.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5fb0e3-cdb1-43e4-ad9b-e24adc9b51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 640x640 1 ÊäºÂç∞, 1 Êú™ÊäºÂç∞\n",
      "Speed: 5.7ms pre-process, 344.9ms inference, 10.7ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[4.27367e+02, 4.45427e+02, 4.72278e+02, 5.03631e+02, 9.22218e-01, 1.00000e+00],\n",
      "        [4.27713e+02, 4.46479e+02, 4.73168e+02, 5.03123e+02, 4.53981e-01, 0.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "# Âá¶ÁêÜ„Åï„Çå„ÅüÁîªÂÉè„Çí‰øùÊåÅ„Åô„Çã„É™„Çπ„Éà\n",
    "processed_imgs = []\n",
    "\n",
    "img_size = 640\n",
    "\n",
    "for img_file in img_files:\n",
    "    # ÁîªÂÉè„ÇíÈñã„Åç„ÄÅÊúüÂæÖ„Åô„Çã„Çµ„Ç§„Ç∫„Å´„É™„Çµ„Ç§„Ç∫\n",
    "    img = Image.open(img_file)\n",
    "    img.thumbnail((img_size, img_size))\n",
    "    img = ImageOps.pad(img, size=((img_size, img_size)))\n",
    "    img.save('pad_002.jpg')\n",
    "    processed_imgs.append(img)\n",
    "\n",
    "for img in processed_imgs:\n",
    "    # model.conf = 0.6\n",
    "    result = model(img)\n",
    "    result.print()\n",
    "    print(result.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f48d8a6-af34-4a05-ae98-c4a60b983671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[4.27367e+02, 4.45427e+02, 4.72278e+02, 5.03631e+02, 9.22218e-01, 1.00000e+00],\n",
       "         [4.27713e+02, 4.46479e+02, 4.73168e+02, 5.03123e+02, 4.53981e-01, 0.00000e+00]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c3462-f8bc-42b2-973e-f080ca125404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d90e6e-f461-439b-81a8-4ede15e00057",
   "metadata": {},
   "source": [
    "## Ëá™ÂâçÂâçÂá¶ÁêÜ„ÄÄÔºã„ÄÄbest.pt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda2d807-8cb1-4767-95f4-89106b4f9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-3-10 Python-3.8.18 torch-2.2.1+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model1 = torch.hub.load('./hidden_others/yolov5', 'custom', './hidden_others/best.pt', source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85fc0787-a15a-45e7-a4c6-0ab173c0ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 640x640 1 ÊäºÂç∞, 1 Êú™ÊäºÂç∞\n",
      "Speed: 11.9ms pre-process, 409.9ms inference, 0.9ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[4.27367e+02, 4.45427e+02, 4.72278e+02, 5.03631e+02, 9.22218e-01, 1.00000e+00],\n",
      "        [4.27713e+02, 4.46479e+02, 4.73168e+02, 5.03123e+02, 4.53982e-01, 0.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "for img in processed_imgs:\n",
    "    # model1.conf = 0.6\n",
    "    result = model1(img)\n",
    "    result.print()\n",
    "    print(result.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba41cf10-88c0-4011-9bf5-ac69e72c4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[4.27367e+02, 4.45427e+02, 4.72278e+02, 5.03631e+02, 9.22218e-01, 1.00000e+00],\n",
      "        [4.27713e+02, 4.46479e+02, 4.73168e+02, 5.03123e+02, 4.53982e-01, 0.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "print(result.xyxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b40c2-aebd-4622-aee9-6dbf82d4db5a",
   "metadata": {},
   "source": [
    "## yolov5 + best.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18944730-a90b-460e-9bf6-82e99dadae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-3-10 Python-3.8.18 torch-2.2.1+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model1 = torch.hub.load('./hidden_others/yolov5', 'custom', './hidden_others/best.pt', source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b27ec55-cdc6-4916-a83a-d1e92bf2f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 1161x819 1 ÊäºÂç∞, 1 Êú™ÊäºÂç∞\n",
      "Speed: 63.9ms pre-process, 399.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[6.04151e+02, 8.07996e+02, 6.85716e+02, 9.13076e+02, 9.32026e-01, 1.00000e+00],\n",
       "         [6.01491e+02, 8.06746e+02, 6.88052e+02, 9.11995e+02, 7.41273e-01, 0.00000e+00]])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model1(img_files)\n",
    "# model1.conf = 0.6\n",
    "result.print()\n",
    "result.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2ae572f8-e746-426b-b860-35583579b8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         xmin        ymin        xmax        ymax  confidence  class name\n",
       " 0  604.151001  807.996033  685.716492  913.076233    0.932026      1  Êú™ÊäºÂç∞\n",
       " 1  601.491211  806.746277  688.052124  911.995056    0.741273      0   ÊäºÂç∞]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.pandas().xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121b39a-0500-4850-97c9-b0d1a35e1424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6b2a3-356c-466e-a0e6-6f7ab7e23beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8550e8-4cb8-4e79-a1a0-e4d7b2d882e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094bbbd2-f145-4d3a-b3b9-615d7609e31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0483d-afd2-4698-accc-c35b836f3cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9996e1-5337-4491-8b93-7460071d1147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c0896-8997-4e53-9b54-c53489542537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4460c90-d335-4ca6-947a-8b74f26b5fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "445bd8a4-1617-40b6-a7ac-c9394d1953c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=640):\n",
    "    image_list = []\n",
    "    for one_image in image_path:\n",
    "        image = Image.open(one_image)\n",
    "        image.thumbnail((target_size, target_size))\n",
    "        padded_image = ImageOps.pad(image, (target_size, target_size), color='black') \n",
    "        image_np = np.array(padded_image).astype(np.float32) / 255.0\n",
    "        image_np = np.transpose(image_np, [2, 0, 1])  # HWC„Åã„ÇâCHW„Å∏Â§âÊèõ\n",
    "        image_np = np.expand_dims(image_np, axis=0)  # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅÆÊ¨°ÂÖÉ„ÇíËøΩÂä†\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dff0e07a-b22d-43a3-9cc0-889e0c6cc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=640):\n",
    "    image_list = []\n",
    "    for one_image in image_path:\n",
    "        image = Image.open(one_image)\n",
    "        image.thumbnail((target_size, target_size))\n",
    "        padded_image = ImageOps.pad(image, (target_size, target_size), color='black') \n",
    "        image_np = ((np.array(padded_image).astype(np.float32) / 255.0)-0.5)*2\n",
    "        image_np = np.transpose(image_np, [2, 0, 1])  # HWC„Åã„ÇâCHW„Å∏Â§âÊèõ\n",
    "        image_np = np.expand_dims(image_np, axis=0)  # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅÆÊ¨°ÂÖÉ„ÇíËøΩÂä†\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9aadb04-bc5b-4638-8b72-1f6e2f555883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX Runtime„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰ΩúÊàê„Åó„ÄÅ„É¢„Éá„É´„Çí„É≠„Éº„Éâ\n",
    "session = onnxruntime.InferenceSession('best.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f93f6a78-096e-4d6f-8c02-6d449c230f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = img_files\n",
    "input_tensor = preprocess_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "84238635-d135-43bc-863d-9dabbb6c6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êé®Ë´ñ„ÇíÂÆüË°å\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "result = session.run([output_name], {input_name: input_tensor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d49f45f-1e51-4477-b860-cdffbd253933",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = 0.25  # ‰ø°È†ºÂÄ§„ÅÆÈñæÂÄ§\n",
    "iou_thres = 0.45  # NMS IOU „ÅÆÈñæÂÄ§\n",
    "agnostic_nms = False\n",
    "max_det = 1000\n",
    "\n",
    "\n",
    "pred = torch.from_numpy(result[0])\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, agnostic_nms, max_det=max_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9bd1359c-254c-4487-91da-15c2e8441943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[4.27126e+02, 4.45251e+02, 4.72056e+02, 5.03764e+02, 9.26464e-01, 1.00000e+00],\n",
       "         [4.27496e+02, 4.46230e+02, 4.73546e+02, 5.03092e+02, 4.41865e-01, 0.00000e+00]])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d5fbb-91dd-41b9-a81a-a6e4dbdaaae9",
   "metadata": {},
   "source": [
    "#### conf_thres„Çí‰∏ã„Åí„Çã„Å®ÊäºÂç∞Ê¨Ñ„ÅÆÊ§úÁü•„Åå„Åï„Çå„ÇãÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29f3fc-ad89-4153-89c2-9327996b93a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1cd9e-26ae-49a3-b408-aa13b690ef37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9c2f9-22e6-4106-ae12-e51f2a0db4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12739bfc-240d-43a3-9de3-b68e4e79bb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d9d52-82d1-4944-a7ac-34fbce74e1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b6096-595c-433d-bd6a-5fcd8cd2a8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409cfa9-15bd-4fc7-a278-264933c00847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67d44e-1790-435d-88b2-07e9f64fb821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba9ea9-4515-426d-b09e-8029f2cf90dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d8d0f-e9f2-4dfc-88f7-520c8100bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34eadff4-3627-40d9-ada8-2832e880cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_label (\n",
    "    img,\n",
    "    box,\n",
    "    label =\"\",\n",
    "    color = (56, 56, 255),\n",
    "    txt_color = (255, 255, 255),\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„Å´„ÇØ„É©„Çπ„ÄÅ„ÇØ„É©„ÇπÁ¢∫Áéá„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Åå‰ªò‰∏é„Åï„Çå„Åübbox„ÇíÊèèÁîª„Åô„Çã„ÄÇ\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    img : (ndarray) cv2Âá¶ÁêÜ„Åï„Çå„Åü„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆndarrayÈÖçÂàó„ÄÇ\n",
    "                    Channel„ÅØ(B, G, R)„ÄÇ\n",
    "    box : (tensor) „É™„Çπ„Ç±„Éº„É´ÁîªÂÉè„ÅÆbbox„ÅÆÊÉÖÂ†±„ÄÄ[x1, y1, x2, y2]„ÄÇ\n",
    "                   x1,y1„ÅØbbox„ÅÆÂ∑¶‰∏ä„ÅÆx,yÂ∫ßÊ®ô„ÄÅx2, y2„ÅØÂè≥‰∏ã„ÅÆx,yÂ∫ßÊ®ô„ÇíË°®„Åô„ÄÇ\n",
    "    label : (str) bbox„Å´‰ªò‰∏é„Åï„Çå„Çã„ÉÜ„Ç≠„Çπ„Éà \"bbox„ÅÆ„ÇØ„É©„ÇπÂêç „ÇØ„É©„ÇπÁ¢∫Áéá(‰∏ã2Ê°Å)\"„ÅÆÊñáÂ≠óÂàó„ÄÇ\n",
    "    color : (tuple | list-like) bbox„ÅÆÊû†„ÅÆËâ≤ (B, G, R)„ÄÇ\n",
    "    txt_color : (tuple | list-like) bbox„ÅÆ‰ªò‰∏é„Åô„Çã„ÉÜ„Ç≠„Çπ„Éà„ÅÆËâ≤„ÄÇ \n",
    "                                    PILÂá¶ÁêÜ„ÅÆ„Åü„ÇÅ„ÄÅChannel„ÅØ(R, G, B)„ÄÇ\n",
    "\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (ndarray) „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„Å´bbox(„ÉÜ„Ç≠„Çπ„Éà‰ªò„Åç)„ÇíÊèèÁîª„Åó„ÅündarrayÈÖçÂàó„ÄÇ\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # YOLOv5„ÅÆ‰ΩøÁî®„Çà„Çä (ÂèÇÁÖßÔºöultralytics/ultralytics/utils/plotting.py)\n",
    "    # line_width„ÅØ„ÄÄ1„ÄÄ„Å®„ÄÄ2„ÅßÊåôÂãï„ÇíÁ¢∫Ë™çÊ∏à„Åø\n",
    "    line_width = max(math.floor(sum(img.shape) / 2 * 0.003), 2) # bbox„ÅÆÊû†Áî®„Å´Ë™øÊï¥\n",
    "    \n",
    "    # p1:(x1, y1), p2:(x2, y2)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    \n",
    "    # bbox„ÅÆÊèèÁîª\n",
    "    cv2.rectangle(img, p1, p2, color, thickness=line_width, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    if label:\n",
    "        \n",
    "        # Êó•Êú¨Ë™û„Éï„Ç©„É≥„Éà„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
    "        font_path = \"font/Koruri-Bold.ttf\"\n",
    "    \n",
    "        # Êó•Êú¨Ë™û„Éï„Ç©„É≥„Éà„ÇíË™≠„ÅøËæº„Åø\n",
    "        font_size = int(line_width * 6)\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "        # „ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Çµ„Ç§„Ç∫„ÇíË®àÁÆó\n",
    "        text_bbox = font.getbbox(label) # return -> bbox„ÅÆ(left, top, right, bottom) \n",
    "        w, h = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "        # „ÉÜ„Ç≠„Çπ„ÉàÁî®„ÅÆÈ†òÂüüÊèèÁîª\n",
    "        outside = p1[1] - h >= 3\n",
    "        p2 = p1[0] + w + 7, p1[1] - h - 5 if outside else p1[1] + h + 5 # „ÉÜ„Ç≠„Çπ„ÉàÂàÜ„ÅÆÂè≥‰∏äÂ∫ßÊ®ô„ÇíË®àÁÆó \n",
    "        cv2.rectangle(img, p1, p2, color, -1, cv2.LINE_AA)  # „ÉÜ„Ç≠„Çπ„ÉàÈ†òÂüü„ÅÆÂ°ó„ÇäÊΩ∞„Åó\n",
    "    \n",
    "        #------------ PIL (BGR -> RGB)„ÄÄ-------------\n",
    "        # PIL„Ç§„É°„Éº„Ç∏„Å´Â§âÊèõ\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGBÂ§âÊèõ\n",
    "        img_pil = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "    \n",
    "        # „ÉÜ„Ç≠„Çπ„Éà„ÇíÊèèÁîª\n",
    "        position = (p1[0]+ 4, p2[1] if outside else p1[1] + h + 2)\n",
    "        draw.text(position, label, font=font, fill=txt_color)\n",
    "    \n",
    "        # ndarray„Å´Â§âÊèõ\n",
    "        img = np.array(img_pil)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR„Å´Êàª„Åô\n",
    "        \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81901027-9002-4100-8a0b-f6e67a940d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_boxes(scaled_img_shape, boxes, img_shape):\n",
    "    \"\"\"\n",
    "    Ê∏°„Åï„Çå„Åübbox„Çí„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉèÁî®„Å´Ë™øÊï¥„Åô„Çã„ÄÇ\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    scaled_img_shape : (tuple) „É™„Çπ„Ç±„Éº„É´„Åó„ÅüÁîªÂÉè„ÅÆshape()\n",
    "    boxes : (tensor) „É™„Çπ„Ç±„Éº„É´ÁîªÂÉèÁî®bbox„ÅÆÊÉÖÂ†±„ÄÄ[x1, y1, x2, y2]\n",
    "                     x1,y1„ÅØbbox„ÅÆÂ∑¶‰∏ä„ÅÆx,yÂ∫ßÊ®ô„ÄÅx2, y2„ÅØÂè≥‰∏ã„ÅÆx,yÂ∫ßÊ®ô„ÇíË°®„Åô\n",
    "    img_shape : (tuple) „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆshape, (H, W, C)„ÅÆÈ†ÜÁï™\n",
    "\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (tensor) „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉèÁî®„Å´Ë™øÊï¥„Åó„Åübbox„ÅÆÊÉÖÂ†± [x1, y1, x2, y2]\n",
    "                      „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„Åã„Çâ„ÅØ„ÅøÂá∫„ÇãÂ§ß„Åç„Åï„ÅÆbbox„ÅØ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„Åï„Çå„Çã„ÄÇ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ratio = min(scaled_img_shape[0] / img_shape[0], scaled_img_shape[1] / img_shape[1]) # new / old\n",
    "    pad = (scaled_img_shape[1] - img_shape[1] * ratio) / 2, (scaled_img_shape[0] - img_shape[0] * ratio) / 2 # ÁâáÈù¢„ÅÆpadË®àÁÆó \n",
    "    \n",
    "    # ÂÖÉÁîªÂÉèÁî®„Å´bbox„ÇíË™øÊï¥„Åô„Çã„Åü„ÇÅ,\n",
    "    # ÁâáÊñπ„ÅÆpadÂàÜ„ÇíÂºï„Åç„ÄÅ„É™„Çπ„Ç±„Éº„É´„ÅÆÊØîÁéá„ÅßÂâ≤„Çã\n",
    "    boxes[:, [0, 2]] -= pad[0] # x padding\n",
    "    boxes[:, [1, 3]] -= pad[1] # y padding\n",
    "    boxes /= ratio \n",
    "    \n",
    "    # 0‰ª•‰∏ä„ÄÅ„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆÂπÖ„ÄÅÈ´ò„Åï‰ª•ÂÜÖ„Å´bbox„ÅåÂÖ•„Çã„Çà„ÅÜ„Å´„ÇØ„É™„ÉÉ„Éó\n",
    "    clip_boxes(boxes, img_shape)\n",
    "\n",
    "    return boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd3c109-82ac-4019-a523-382d000469c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    bbox„Åå„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆÁØÑÂõ≤ÂÜÖ„Å´Âèé„Åæ„Çã„Çà„ÅÜ„Å´„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞\n",
    "    0‰ª•‰∏ä„ÇÇ„Åó„Åè„ÅØ„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆÂπÖ„ÄÅÈ´ò„Åï„Å´Âà∂Èôê„Åô„Çã„ÄÇ\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    boxes : (tensor) „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉèÁî®„Å´Ë™øÊï¥„Åó„Åübbox, [x1, y1, x2, y2],\n",
    "                     x1,y1„ÅØbbox„ÅÆÂ∑¶‰∏ä„ÅÆx,yÂ∫ßÊ®ô„ÄÅx2, y2„ÅØÂè≥‰∏ã„ÅÆx,yÂ∫ßÊ®ô„ÇíË°®„Åô\n",
    "    shape : (tuple) „Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆshape, (H, W, C)„ÅÆÈ†ÜÁï™\n",
    "\n",
    "\n",
    "    Return\n",
    "    -----------\n",
    "    output : None \n",
    "             „Åó„Åã„Åó„ÄÅ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„Å´„Çà„ÇäÂ§âÊõ¥„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅÁõ¥Êé•boxes„Å´Áõ¥Êé•Â§âÊõ¥„ÅåÂä†„Åà„Çâ„Çå„Çã„ÄÇ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # bbox„Åå„Ç™„É™„Ç∏„Éä„É´ÁîªÂÉè„ÅÆÁØÑÂõ≤ÂÜÖ„Å´Âèé„Åæ„Çã„Çà„ÅÜ„Å´„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞\n",
    "    boxes[..., 0].clamp_(0, shape[1])  # x1\n",
    "    boxes[..., 1].clamp_(0, shape[0])  # y1\n",
    "    boxes[..., 2].clamp_(0, shape[1])  # x2\n",
    "    boxes[..., 3].clamp_(0, shape[0])  # y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2298587-2150-40d3-8ee3-037ad87eb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "    prediction, \n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    agnostic=False,\n",
    "    labels=(),\n",
    "    max_det=300,\n",
    "    nm=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    YOLO„ÅÆ‰∫àÊ∏¨ÁµêÊûú„ÇíNMSÂá¶ÁêÜ„Åô„Çã„ÄÇ\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    prediction : (tensor) „ÉÜ„É≥„ÇΩ„É´Âåñ„Åó„Åüyolo„Åå‰∫àÊ∏¨„Åó„ÅüÁµêÊûú\n",
    "    conf_thres : (float) „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ(bbox)„ÅÆ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„ÅÆÈñæÂÄ§\n",
    "    iou_thres : (float) IOU„ÅÆÈñæÂÄ§\n",
    "    agnostic : (Bool) Èáç„Å™„Å£„ÅüÁï∞„Å™„Çã„ÇØ„É©„Çπ„ÅÆbbox„ÇíÂêå‰∏Ä„ÅÆbbox„Å´„Åô„Çã„Åã\n",
    "    labels : (tuple or list-like) ÁîªÂÉèÂÜÖ„ÅÆ„É©„Éô„É´„ÅÆÊÉÖÂ†±\n",
    "    max_det : (int) ÊúÄÂ§ß„ÅÆbboxÊï∞\n",
    "    nm : (int) „Éû„Çπ„ÇØÊï∞\n",
    "    \n",
    "    Return\n",
    "    -----------\n",
    "    output : (tensor) NMSÂá¶ÁêÜÂæå„ÅÆbboxÊÉÖÂ†±„ÇíÂê´„ÇÄ„ÉÜ„É≥„ÇΩ„É´„ÄÅ[x1, y1, x2, y2, „ÇØ„É©„ÇπÁ¢∫Áéá, cls_id]„ÅÆ„Ç´„É©„É†„Å´Â§âÊèõ„Åï„Çå„Å¶„ÅÑ„Çã\n",
    "                      x1,y1„ÅØbbox„ÅÆÂ∑¶‰∏ä„ÅÆx,yÂ∫ßÊ®ô„ÄÅx2, y2„ÅØÂè≥‰∏ã„ÅÆx,yÂ∫ßÊ®ô„ÇíË°®„Åô\n",
    "                      \n",
    "    \"\"\"\n",
    "    batch_size = prediction.shape[0] # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫\n",
    "    num_class = prediction.shape[2] - nm - 5 # „ÇØ„É©„ÇπÊï∞\n",
    "    bool_cnf = prediction[..., 4] > conf_thres # bboxÊØé„Å´‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„Ååconf_thres„Çà„ÇäÂ§ß„Åç„ÅÑ„Åã„ÅÆbool\n",
    "    \n",
    "    max_wh = 7680  # ÊúÄÂ§ß„ÅÆbbox„ÅÆÂπÖ„ÄÅÈ´ò„Åï\n",
    "    max_nms = 30000  # torchvision.ops.nms()„ÅÆ„Åü„ÇÅ„ÅÆÊúÄÂ§ß„ÅÆbboxÊï∞\n",
    "    time_limit = 0.5 + 0.05 * batch_size  # „Çø„Ç§„É†„É™„Éü„ÉÉ„Éà(s)\n",
    "    \n",
    "    start = time.time()\n",
    "    mi = 5 + num_class\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * batch_size\n",
    "    \n",
    "    \n",
    "    for idx, x in enumerate(prediction):\n",
    "    \n",
    "        # conf_thres„Çà„ÇäÂ§ß„Åç„ÅÑ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢„ÇíÊåÅ„Å§bboxs„ÇíÊäΩÂá∫\n",
    "        x = x[bool_cnf[idx]]\n",
    "    \n",
    "        # labels„ÇíÊåÅ„Å£„Å¶„ÅÑ„ÅüÊôÇ„ÅÆÂá¶ÁêÜ\n",
    "        if labels and len(labels[idx]):\n",
    "                lb = labels[idx]\n",
    "                v = torch.zeros((len(lb), num_class + nm + 5), device=x.device)\n",
    "                v[:, :4] = lb[:, 1:5]  # box\n",
    "                v[:, 4] = 1.0  # conf\n",
    "                v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "                x = torch.cat((x, v), 0)\n",
    "            \n",
    "    \n",
    "        # „Éï„Ç£„É´„Çø„ÉºÂæå„ÅÆbbox„Åå„Å™„ÅÑÂ†¥Âêà\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "            \n",
    "        x[:, 5:] *= x[:, 4:5] # ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢*ÂêÑclsÁ¢∫Áéá\n",
    "        box = xywh2xyxy(x[:, :4]) # [x1, y1, x2, y2]:bbox„ÅÆÂ∑¶‰∏ä„Å®Âè≥‰∏ã„ÅÆÂ∫ßÊ®ô„Å´Â§âÊèõ\n",
    "        mask = x[:, mi:] # mask„Åå„Å™„Åë„Çå„Å∞[]\n",
    "    \n",
    "        cls_prob, cls_id = x[:, 5:mi].max(1, keepdim=True) # ÂêÑbbox„ÅßÁ¢∫Áéá„ÅÆÈ´ò„ÅÑcls„ÅÆÂÄ§„Å®cls_id„ÇíËøî„Åô\n",
    "    \n",
    "        # [x1, y1, x2, y2, cls„ÅÆÁ¢∫Áéá, cls_id]„ÅÆ„Ç´„É©„É†„ÅÆÈ†Ü„Å´ÁµêÂêà\n",
    "        # cls„ÅÆÁ¢∫Áéá(Êú¨Êù•„ÅÆprob*‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢)„Ååconf_thres„Çà„ÇäÈ´ò„ÅÑ„ÇÇ„ÅÆ„ÅÆ„ÅøÊäΩÂá∫\n",
    "        x = torch.cat((box, cls_prob, cls_id.float(), mask), axis=1)[cls_prob.view(-1) > conf_thres]\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "    \n",
    "        # clsÁ¢∫Áéá„Çí„Ç≠„Éº„Å´„Åó„Å¶ÈôçÈ†Ü„Å´‰∏¶„Å≥Êõø„Åà„ÄÅ‰∏î„Å§ max_nms„ÇíË∂Ö„Åà„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]] \n",
    "    \n",
    "        # Áï∞„Å™„Çã„ÇØ„É©„Çπ„ÅÆbbox„ÇíÂå∫Âà•„Åó„Å¶Ë©ï‰æ°„Åô„Çã„Åü„ÇÅ„ÄÅagnostic„Åß„ÇØ„É©„ÇπÂõ∫Êúâ„ÅÆbbox„Çí‰ΩúÊàê\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "    \n",
    "        # nms\n",
    "        selected_idx = torchvision.ops.nms(boxes, scores, iou_thres)\n",
    "        selected_idx = selected_idx[:max_det]\n",
    "\n",
    "        output[idx] = x[selected_idx]\n",
    "        finish = time.time()\n",
    "        if (finish - start) > time_limit:\n",
    "            break\n",
    "            \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "683b3cfb-9bb1-4c77-8414-ca862c73f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    [x‰∏≠ÂøÉ„ÄÅy‰∏≠ÂøÉ„ÄÅbbox„ÅÆÂπÖ, bbox„ÅÆÈ´ò„Åï] -> [bbox„ÅÆÂ∑¶‰∏ä„ÅÆx„ÄÅbbox„ÅÆÂ∑¶‰∏ä„ÅÆy, bbox„ÅÆÂè≥‰∏ã„ÅÆx, bbox„ÅÆÂè≥‰∏ã„ÅÆy]\n",
    "    \n",
    "    \"\"\"\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # bbox„ÅÆÂ∑¶‰∏ä„ÅÆx\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # bbox„ÅÆÂ∑¶‰∏ä„ÅÆy\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bbox„ÅÆÂè≥‰∏ã„ÅÆx\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bbox„ÅÆÂè≥‰∏ã„ÅÆy\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f6b58b0-ed3c-418f-8bae-f9992fa9be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(\n",
    "    img,\n",
    "    new_shape=(640, 640),\n",
    "    color=(0, 0, 0),\n",
    "    scaleup=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    ÊåáÂÆö„Åï„Çå„Åü„Çµ„Ç§„Ç∫„Å´ÁîªÂÉè„Çí„É™„Çµ„Ç§„Ç∫„ÇíË°å„ÅÜ„ÄÇ\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    img : (ndarray) ÁîªÂÉè„ÅÆndarrayÈÖçÂàó\n",
    "    new_shape : (tuple) „É™„Çµ„Ç§„Ç∫„Åó„Åü„ÅÑÁîªÂÉè„Çµ„Ç§„Ç∫\n",
    "    color : (tuple:(B, G, R)) „Éë„Éá„Ç£„É≥„Ç∞„ÅÆËâ≤\n",
    "    scaleup : (Bool) „Çπ„Ç±„Éº„É´„Ç¢„ÉÉ„Éó„ÇíË°å„ÅÜÂ†¥Âêà„ÅØTrue\n",
    "\n",
    "    Return\n",
    "    -----------\n",
    "    padded_img : (ndarray) „É™„Çµ„Ç§„Ç∫„Åï„Çå„ÅüÁîªÂÉè„ÅÆndarrayÈÖçÂàó\n",
    "    ratio : (tuple „É™„Çµ„Ç§„Ç∫„Åó„ÅüÊØîÁéá\n",
    "    (dw, dh) : (int:(Ê®™„ÄÅÈ´ò„Åï)) „Éë„Éá„Ç£„É≥„Ç∞„Åó„ÅüÊï∞ÂÄ§(Ê®™„ÄÅÈ´ò„Åï)\n",
    "    \"\"\"\n",
    "    \n",
    "    # „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Çí‰øù„Å£„ÅüÁîªÂÉè„ÅÆ„É™„Çµ„Ç§„Ç∫\n",
    "    ori_shape = img.shape[:2] # [H, W, C] -> [H, W]\n",
    "    r = min(new_shape[0] / ori_shape[0], new_shape[1] / ori_shape[1])\n",
    "    \n",
    "    # „Çπ„Ç±„Éº„É´„Ç¢„ÉÉ„Éó„Å™„Åó„ÅÆÂ†¥Âêà\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "    \n",
    "    # „Ç¢„Çπ„Éö„ÇØ„ÉàÊØî„Çí‰øù„Å£„Åü„Åæ„Åæ„ÄÅ„É™„Çπ„Ç±„Éº„É´\n",
    "    ratio = (r, r)\n",
    "    new_unpad = (round(ori_shape[1] * r), round(ori_shape[0] * r)) # cv.resize„ÅÆ„Åü„ÇÅ[W, H]„Å´„Åô„Çã\n",
    "    \n",
    "    # „Éë„Éá„Ç£„É≥„Ç∞„Åô„ÇãÈ†òÂüü„ÇíÁÆóÂá∫\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw, dh = dw / 2, dh / 2\n",
    "\n",
    "    # dw, df„Åå.5„ÅÆÊôÇ„ÅÆÂØæÂøú -> „Åì„ÅÆÂæÆË™øÊï¥„Çí„Åó„Å™„ÅÑ„Å®session.run()ÊôÇ„ÅÆinput„ÅÆshape„ÅåÂêà„Çè„Å™„Åè„Å™„Çã\n",
    "    top, bottom = round(dh - 0.1), round(dh + 0.1)\n",
    "    left, right = round(dw - 0.1), round(dw + 0.1)\n",
    "    \n",
    "    # ÁîªÂÉè„ÅÆ„É™„Çµ„Ç§„Ç∫\n",
    "    if new_shape[::-1] != new_unpad:\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "    return padded_img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ea9ce-a781-4d85-8450-7b54c8545cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2a132-35eb-4e91-8e2d-4cad8609e29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04d86b-97ad-4d27-b59f-6bf646edbb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b8408-de89-4181-8cb4-3ec92b5f7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec6f21-e0bf-4193-9901-3422e1b6b0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383189b3-a281-4b18-a61f-a85101800d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e8215-ff96-4f2e-94d5-64943b74d4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d7fc3-ee8d-43a0-a89f-d901a0f76f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
